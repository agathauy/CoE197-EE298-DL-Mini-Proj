{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 250, 512)          1054720   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1024)              4202496   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,913,601\n",
      "Trainable params: 5,913,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               256256    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 250)               256250    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 250, 1)            0         \n",
      "=================================================================\n",
      "Total params: 1,176,570\n",
      "Trainable params: 1,172,986\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "[train]: Starting...\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Approaching Champion Cynthia.mid\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Battle Champion.mid\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Battle Team Galactic Admin.mid\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Battle Rival.mid\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Azelf Mesprit and Uxie Battle.mid\n",
      "0 [D loss: 0.699807, acc.: 12.50%] [G loss: 0.688476] [time: 6.373056]\n",
      "1 [D loss: 0.682862, acc.: 64.06%] [G loss: 0.688577] [time: 6.610731]\n",
      "2 [D loss: 0.665834, acc.: 71.88%] [G loss: 0.688861] [time: 6.833201]\n",
      "3 [D loss: 0.652700, acc.: 64.06%] [G loss: 0.704326] [time: 7.048347]\n",
      "4 [D loss: 0.572602, acc.: 73.44%] [G loss: 0.887642] [time: 7.269692]\n",
      "5 [D loss: 1.081557, acc.: 65.62%] [G loss: 1.038690] [time: 7.489516]\n",
      "6 [D loss: 0.372774, acc.: 87.50%] [G loss: 1.007672] [time: 7.710196]\n",
      "7 [D loss: 0.444471, acc.: 90.62%] [G loss: 1.201637] [time: 7.929563]\n",
      "8 [D loss: 0.444674, acc.: 85.94%] [G loss: 1.443068] [time: 8.154648]\n",
      "9 [D loss: 0.329684, acc.: 90.62%] [G loss: 2.180644] [time: 8.373958]\n",
      "10 [D loss: 0.253933, acc.: 87.50%] [G loss: 3.657115] [time: 8.595761]\n",
      "11 [D loss: 0.099024, acc.: 93.75%] [G loss: 6.195123] [time: 8.815452]\n",
      "12 [D loss: 0.843688, acc.: 82.81%] [G loss: 5.541188] [time: 9.037338]\n",
      "13 [D loss: 0.582936, acc.: 79.69%] [G loss: 1.843560] [time: 9.256639]\n",
      "14 [D loss: 0.271360, acc.: 90.62%] [G loss: 2.555080] [time: 9.477750]\n",
      "15 [D loss: 0.146694, acc.: 96.88%] [G loss: 3.961971] [time: 9.695579]\n",
      "16 [D loss: 0.467380, acc.: 84.38%] [G loss: 2.675753] [time: 9.913609]\n",
      "17 [D loss: 0.224972, acc.: 92.19%] [G loss: 2.451726] [time: 10.135607]\n",
      "18 [D loss: 0.272456, acc.: 90.62%] [G loss: 2.575971] [time: 10.353189]\n",
      "19 [D loss: 0.228318, acc.: 92.19%] [G loss: 3.589204] [time: 10.574980]\n",
      "20 [D loss: 0.232102, acc.: 90.62%] [G loss: 2.793342] [time: 10.791847]\n",
      "21 [D loss: 0.304775, acc.: 87.50%] [G loss: 3.202829] [time: 11.013108]\n",
      "22 [D loss: 0.331753, acc.: 84.38%] [G loss: 3.184620] [time: 11.229441]\n",
      "23 [D loss: 0.168388, acc.: 92.19%] [G loss: 3.797222] [time: 11.450672]\n",
      "24 [D loss: 0.316287, acc.: 84.38%] [G loss: 3.925851] [time: 11.672681]\n",
      "25 [D loss: 0.322143, acc.: 85.94%] [G loss: 3.775570] [time: 11.889831]\n",
      "26 [D loss: 0.420523, acc.: 79.69%] [G loss: 2.441257] [time: 12.112203]\n",
      "27 [D loss: 0.324217, acc.: 87.50%] [G loss: 3.386104] [time: 12.333444]\n",
      "28 [D loss: 0.490117, acc.: 78.12%] [G loss: 1.796546] [time: 12.553300]\n",
      "29 [D loss: 0.306406, acc.: 89.06%] [G loss: 1.980550] [time: 12.772684]\n",
      "30 [D loss: 0.419448, acc.: 79.69%] [G loss: 2.067380] [time: 12.993607]\n",
      "31 [D loss: 0.247565, acc.: 90.62%] [G loss: 2.371779] [time: 13.214379]\n",
      "32 [D loss: 0.236374, acc.: 90.62%] [G loss: 2.786300] [time: 13.432636]\n",
      "33 [D loss: 0.346810, acc.: 82.81%] [G loss: 2.952554] [time: 13.652992]\n",
      "34 [D loss: 0.189414, acc.: 93.75%] [G loss: 3.832749] [time: 13.872557]\n",
      "35 [D loss: 0.226542, acc.: 85.94%] [G loss: 3.886188] [time: 14.088563]\n",
      "36 [D loss: 0.323852, acc.: 89.06%] [G loss: 2.345891] [time: 14.307186]\n",
      "37 [D loss: 0.243221, acc.: 92.19%] [G loss: 2.526017] [time: 14.525948]\n",
      "38 [D loss: 0.349502, acc.: 84.38%] [G loss: 2.649962] [time: 14.744324]\n",
      "39 [D loss: 0.240978, acc.: 92.19%] [G loss: 3.169120] [time: 14.962605]\n",
      "40 [D loss: 0.356782, acc.: 85.94%] [G loss: 2.148956] [time: 15.178937]\n",
      "41 [D loss: 0.330730, acc.: 79.69%] [G loss: 2.310265] [time: 15.401063]\n",
      "42 [D loss: 0.310284, acc.: 87.50%] [G loss: 2.661868] [time: 15.617710]\n",
      "43 [D loss: 0.376420, acc.: 84.38%] [G loss: 2.950145] [time: 15.838015]\n",
      "44 [D loss: 0.583603, acc.: 78.12%] [G loss: 2.031429] [time: 16.054624]\n",
      "45 [D loss: 0.380185, acc.: 87.50%] [G loss: 1.889984] [time: 16.274743]\n",
      "46 [D loss: 0.363190, acc.: 84.38%] [G loss: 2.044425] [time: 16.494426]\n",
      "47 [D loss: 0.328591, acc.: 85.94%] [G loss: 2.237686] [time: 16.715097]\n",
      "48 [D loss: 0.354810, acc.: 84.38%] [G loss: 2.665473] [time: 16.936401]\n",
      "49 [D loss: 0.331065, acc.: 85.94%] [G loss: 2.874534] [time: 17.157147]\n",
      "50 [D loss: 0.307370, acc.: 85.94%] [G loss: 2.385197] [time: 17.375814]\n",
      "51 [D loss: 0.374313, acc.: 81.25%] [G loss: 2.386341] [time: 17.592502]\n",
      "52 [D loss: 0.385553, acc.: 85.94%] [G loss: 2.525525] [time: 17.813821]\n",
      "53 [D loss: 0.260223, acc.: 90.62%] [G loss: 2.641556] [time: 18.029795]\n",
      "54 [D loss: 0.323913, acc.: 85.94%] [G loss: 2.953775] [time: 18.248025]\n",
      "55 [D loss: 0.189488, acc.: 90.62%] [G loss: 2.800551] [time: 18.466460]\n",
      "56 [D loss: 0.326533, acc.: 87.50%] [G loss: 3.081279] [time: 18.683824]\n",
      "57 [D loss: 0.175050, acc.: 96.88%] [G loss: 4.214335] [time: 18.905901]\n",
      "58 [D loss: 0.286608, acc.: 85.94%] [G loss: 2.010105] [time: 19.120746]\n",
      "59 [D loss: 0.209725, acc.: 89.06%] [G loss: 2.097147] [time: 19.338861]\n",
      "60 [D loss: 0.355107, acc.: 82.81%] [G loss: 3.100564] [time: 19.556732]\n",
      "61 [D loss: 0.262009, acc.: 89.06%] [G loss: 3.510952] [time: 19.782258]\n",
      "62 [D loss: 0.191753, acc.: 92.19%] [G loss: 5.317500] [time: 20.003236]\n",
      "63 [D loss: 0.526853, acc.: 84.38%] [G loss: 1.614394] [time: 20.223025]\n",
      "64 [D loss: 0.519627, acc.: 70.31%] [G loss: 1.532984] [time: 20.441255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 [D loss: 0.369620, acc.: 81.25%] [G loss: 1.814921] [time: 20.663640]\n",
      "66 [D loss: 0.316493, acc.: 93.75%] [G loss: 2.478543] [time: 20.883535]\n",
      "67 [D loss: 0.293875, acc.: 89.06%] [G loss: 1.606921] [time: 21.106350]\n",
      "68 [D loss: 0.275437, acc.: 89.06%] [G loss: 2.070954] [time: 21.327346]\n",
      "69 [D loss: 0.305383, acc.: 90.62%] [G loss: 2.580058] [time: 21.549499]\n",
      "70 [D loss: 0.168211, acc.: 90.62%] [G loss: 3.409539] [time: 21.771140]\n",
      "71 [D loss: 0.151355, acc.: 93.75%] [G loss: 3.318336] [time: 21.992850]\n",
      "72 [D loss: 0.434066, acc.: 81.25%] [G loss: 2.368668] [time: 22.216938]\n",
      "73 [D loss: 0.282239, acc.: 92.19%] [G loss: 3.631080] [time: 22.440607]\n",
      "74 [D loss: 0.211305, acc.: 92.19%] [G loss: 2.921126] [time: 22.665809]\n",
      "75 [D loss: 0.361103, acc.: 85.94%] [G loss: 2.254417] [time: 22.885453]\n",
      "76 [D loss: 0.745010, acc.: 67.19%] [G loss: 1.337765] [time: 23.104254]\n",
      "77 [D loss: 0.307521, acc.: 89.06%] [G loss: 2.503303] [time: 23.322660]\n",
      "78 [D loss: 0.393842, acc.: 85.94%] [G loss: 5.008885] [time: 23.542052]\n",
      "79 [D loss: 0.940423, acc.: 70.31%] [G loss: 2.668543] [time: 23.764713]\n",
      "80 [D loss: 1.582345, acc.: 10.94%] [G loss: 1.603913] [time: 23.984273]\n",
      "81 [D loss: 0.874434, acc.: 34.38%] [G loss: 2.223759] [time: 24.204594]\n",
      "82 [D loss: 0.447833, acc.: 89.06%] [G loss: 2.945895] [time: 24.422693]\n",
      "83 [D loss: 0.701611, acc.: 67.19%] [G loss: 2.202640] [time: 24.644870]\n",
      "84 [D loss: 0.477158, acc.: 89.06%] [G loss: 1.869530] [time: 24.867233]\n",
      "85 [D loss: 0.469856, acc.: 84.38%] [G loss: 1.962014] [time: 25.092057]\n",
      "86 [D loss: 0.378489, acc.: 87.50%] [G loss: 2.592371] [time: 25.310194]\n",
      "87 [D loss: 0.836032, acc.: 67.19%] [G loss: 1.385858] [time: 25.532415]\n",
      "88 [D loss: 0.609898, acc.: 75.00%] [G loss: 1.000093] [time: 25.752700]\n",
      "89 [D loss: 0.544799, acc.: 76.56%] [G loss: 1.138578] [time: 25.973244]\n",
      "90 [D loss: 0.701138, acc.: 57.81%] [G loss: 0.800788] [time: 26.195482]\n",
      "91 [D loss: 0.764149, acc.: 54.69%] [G loss: 0.794824] [time: 26.417961]\n",
      "92 [D loss: 0.715486, acc.: 51.56%] [G loss: 0.806308] [time: 26.637932]\n",
      "93 [D loss: 0.673864, acc.: 45.31%] [G loss: 0.836700] [time: 26.858872]\n",
      "94 [D loss: 0.652942, acc.: 64.06%] [G loss: 0.803447] [time: 27.081789]\n",
      "95 [D loss: 0.645209, acc.: 70.31%] [G loss: 0.855383] [time: 27.302643]\n",
      "96 [D loss: 0.548584, acc.: 75.00%] [G loss: 0.990666] [time: 27.525757]\n",
      "97 [D loss: 0.679655, acc.: 60.94%] [G loss: 1.064587] [time: 27.743507]\n",
      "98 [D loss: 0.772261, acc.: 54.69%] [G loss: 0.886442] [time: 27.966489]\n",
      "99 [D loss: 0.616269, acc.: 67.19%] [G loss: 0.883759] [time: 28.186338]\n",
      "100 [D loss: 0.622313, acc.: 65.62%] [G loss: 0.918589] [time: 28.408066]\n",
      "101 [D loss: 0.603744, acc.: 62.50%] [G loss: 1.024450] [time: 28.633382]\n",
      "102 [D loss: 0.618079, acc.: 65.62%] [G loss: 1.023565] [time: 28.853117]\n",
      "103 [D loss: 0.615593, acc.: 71.88%] [G loss: 1.051168] [time: 29.073799]\n",
      "104 [D loss: 0.505405, acc.: 79.69%] [G loss: 1.049886] [time: 29.290528]\n",
      "105 [D loss: 0.641296, acc.: 65.62%] [G loss: 1.165519] [time: 29.510821]\n",
      "106 [D loss: 0.680517, acc.: 60.94%] [G loss: 1.112456] [time: 29.730730]\n",
      "107 [D loss: 0.567255, acc.: 68.75%] [G loss: 1.061673] [time: 29.953729]\n",
      "108 [D loss: 0.673147, acc.: 65.62%] [G loss: 1.127876] [time: 30.174495]\n",
      "109 [D loss: 0.625040, acc.: 70.31%] [G loss: 1.255947] [time: 30.395920]\n",
      "110 [D loss: 0.785093, acc.: 40.62%] [G loss: 1.161551] [time: 30.615008]\n",
      "111 [D loss: 0.728994, acc.: 32.81%] [G loss: 1.656957] [time: 30.834965]\n",
      "112 [D loss: 0.625251, acc.: 68.75%] [G loss: 1.121848] [time: 31.056071]\n",
      "113 [D loss: 0.656719, acc.: 64.06%] [G loss: 0.868129] [time: 31.275997]\n",
      "114 [D loss: 0.604701, acc.: 68.75%] [G loss: 1.092231] [time: 31.493680]\n",
      "115 [D loss: 0.670366, acc.: 59.38%] [G loss: 1.039615] [time: 31.712781]\n",
      "116 [D loss: 0.641573, acc.: 59.38%] [G loss: 0.997610] [time: 31.936705]\n",
      "117 [D loss: 0.642520, acc.: 65.62%] [G loss: 1.071416] [time: 32.154294]\n",
      "118 [D loss: 0.658356, acc.: 56.25%] [G loss: 1.110002] [time: 32.370337]\n",
      "119 [D loss: 0.625518, acc.: 71.88%] [G loss: 1.297465] [time: 32.589050]\n",
      "120 [D loss: 0.420385, acc.: 85.94%] [G loss: 1.654005] [time: 32.804016]\n",
      "121 [D loss: 0.585391, acc.: 64.06%] [G loss: 3.098516] [time: 33.021029]\n",
      "122 [D loss: 1.647750, acc.: 20.31%] [G loss: 0.638342] [time: 33.238488]\n",
      "123 [D loss: 0.691853, acc.: 48.44%] [G loss: 0.682409] [time: 33.453862]\n",
      "124 [D loss: 0.757782, acc.: 48.44%] [G loss: 0.614397] [time: 33.674933]\n",
      "125 [D loss: 0.682763, acc.: 60.94%] [G loss: 0.632754] [time: 33.891071]\n",
      "126 [D loss: 0.613838, acc.: 68.75%] [G loss: 0.640393] [time: 34.111903]\n",
      "127 [D loss: 0.576175, acc.: 79.69%] [G loss: 0.668968] [time: 34.329926]\n",
      "128 [D loss: 0.605790, acc.: 67.19%] [G loss: 0.726290] [time: 34.547077]\n",
      "129 [D loss: 0.469634, acc.: 79.69%] [G loss: 0.893875] [time: 34.764668]\n",
      "130 [D loss: 0.442512, acc.: 81.25%] [G loss: 1.004330] [time: 34.981171]\n",
      "131 [D loss: 0.606247, acc.: 65.62%] [G loss: 0.711179] [time: 35.201890]\n",
      "132 [D loss: 0.534425, acc.: 76.56%] [G loss: 0.778458] [time: 35.423469]\n",
      "133 [D loss: 0.478786, acc.: 79.69%] [G loss: 0.972090] [time: 35.647770]\n",
      "134 [D loss: 0.574732, acc.: 73.44%] [G loss: 0.920484] [time: 35.867166]\n",
      "135 [D loss: 0.471557, acc.: 78.12%] [G loss: 1.205658] [time: 36.085142]\n",
      "136 [D loss: 0.359708, acc.: 87.50%] [G loss: 1.556228] [time: 36.305026]\n",
      "137 [D loss: 0.634086, acc.: 71.88%] [G loss: 1.258641] [time: 36.523267]\n",
      "138 [D loss: 0.515218, acc.: 70.31%] [G loss: 1.297274] [time: 36.743726]\n",
      "139 [D loss: 0.544438, acc.: 70.31%] [G loss: 1.598186] [time: 36.960320]\n",
      "140 [D loss: 0.448986, acc.: 79.69%] [G loss: 1.404949] [time: 37.182143]\n",
      "141 [D loss: 0.556050, acc.: 71.88%] [G loss: 1.376405] [time: 37.402020]\n",
      "142 [D loss: 0.523084, acc.: 75.00%] [G loss: 1.267456] [time: 37.623217]\n",
      "143 [D loss: 0.528904, acc.: 81.25%] [G loss: 1.341842] [time: 37.841195]\n",
      "144 [D loss: 0.389641, acc.: 82.81%] [G loss: 2.239506] [time: 38.062570]\n",
      "145 [D loss: 0.476921, acc.: 82.81%] [G loss: 1.843351] [time: 38.280880]\n",
      "146 [D loss: 0.523422, acc.: 81.25%] [G loss: 1.544818] [time: 38.498870]\n",
      "147 [D loss: 0.538192, acc.: 81.25%] [G loss: 1.567302] [time: 38.721502]\n",
      "148 [D loss: 0.495990, acc.: 75.00%] [G loss: 1.425206] [time: 38.939877]\n",
      "149 [D loss: 0.540840, acc.: 76.56%] [G loss: 1.154868] [time: 39.162229]\n",
      "150 [D loss: 0.613324, acc.: 75.00%] [G loss: 1.201984] [time: 39.381606]\n",
      "151 [D loss: 0.503335, acc.: 81.25%] [G loss: 1.242826] [time: 39.603415]\n",
      "152 [D loss: 0.518097, acc.: 76.56%] [G loss: 1.376356] [time: 39.825670]\n",
      "153 [D loss: 0.493951, acc.: 76.56%] [G loss: 1.311414] [time: 40.048373]\n",
      "154 [D loss: 0.576565, acc.: 71.88%] [G loss: 1.031019] [time: 40.269464]\n",
      "155 [D loss: 0.590168, acc.: 67.19%] [G loss: 1.174601] [time: 40.488684]\n",
      "156 [D loss: 0.584452, acc.: 67.19%] [G loss: 1.223229] [time: 40.709825]\n",
      "157 [D loss: 0.537310, acc.: 76.56%] [G loss: 1.252816] [time: 40.926917]\n",
      "158 [D loss: 0.528665, acc.: 75.00%] [G loss: 0.968527] [time: 41.150683]\n",
      "159 [D loss: 0.513695, acc.: 78.12%] [G loss: 1.193370] [time: 41.371881]\n",
      "160 [D loss: 0.643116, acc.: 56.25%] [G loss: 1.204224] [time: 41.592106]\n",
      "161 [D loss: 0.529415, acc.: 76.56%] [G loss: 1.327805] [time: 41.811904]\n",
      "162 [D loss: 0.565935, acc.: 68.75%] [G loss: 1.572937] [time: 42.030548]\n",
      "163 [D loss: 0.558559, acc.: 76.56%] [G loss: 1.721902] [time: 42.246937]\n",
      "164 [D loss: 0.466817, acc.: 75.00%] [G loss: 1.558983] [time: 42.466540]\n",
      "165 [D loss: 0.460160, acc.: 82.81%] [G loss: 1.908817] [time: 42.685533]\n",
      "166 [D loss: 0.711408, acc.: 75.00%] [G loss: 1.539425] [time: 42.903831]\n",
      "167 [D loss: 0.510024, acc.: 84.38%] [G loss: 1.296952] [time: 43.123616]\n",
      "168 [D loss: 0.536279, acc.: 76.56%] [G loss: 1.533465] [time: 43.342472]\n",
      "169 [D loss: 0.604388, acc.: 75.00%] [G loss: 1.132325] [time: 43.562108]\n",
      "170 [D loss: 0.562843, acc.: 76.56%] [G loss: 1.164336] [time: 43.781843]\n",
      "171 [D loss: 0.570168, acc.: 70.31%] [G loss: 1.268568] [time: 44.005545]\n",
      "172 [D loss: 0.622135, acc.: 70.31%] [G loss: 1.108134] [time: 44.224228]\n",
      "173 [D loss: 0.586951, acc.: 67.19%] [G loss: 1.142343] [time: 44.445807]\n",
      "174 [D loss: 0.557681, acc.: 73.44%] [G loss: 1.272080] [time: 44.665695]\n",
      "175 [D loss: 0.556281, acc.: 76.56%] [G loss: 1.229652] [time: 44.886798]\n",
      "176 [D loss: 0.745999, acc.: 56.25%] [G loss: 1.063659] [time: 45.107985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 [D loss: 0.522438, acc.: 73.44%] [G loss: 1.090229] [time: 45.327185]\n",
      "178 [D loss: 0.535070, acc.: 71.88%] [G loss: 1.106449] [time: 45.546826]\n",
      "179 [D loss: 0.553691, acc.: 70.31%] [G loss: 1.270765] [time: 45.767060]\n",
      "180 [D loss: 0.525565, acc.: 76.56%] [G loss: 1.179613] [time: 45.988038]\n",
      "181 [D loss: 0.601176, acc.: 65.62%] [G loss: 1.204076] [time: 46.206748]\n",
      "182 [D loss: 0.577230, acc.: 67.19%] [G loss: 1.221340] [time: 46.429491]\n",
      "183 [D loss: 0.455761, acc.: 78.12%] [G loss: 1.397746] [time: 46.649153]\n",
      "184 [D loss: 0.550279, acc.: 75.00%] [G loss: 1.425408] [time: 46.868196]\n",
      "185 [D loss: 0.695819, acc.: 60.94%] [G loss: 1.237570] [time: 47.090284]\n",
      "186 [D loss: 0.653924, acc.: 64.06%] [G loss: 1.344671] [time: 47.311579]\n",
      "187 [D loss: 0.619045, acc.: 71.88%] [G loss: 1.110773] [time: 47.531186]\n",
      "188 [D loss: 0.578203, acc.: 70.31%] [G loss: 1.202684] [time: 47.752528]\n",
      "189 [D loss: 0.648137, acc.: 67.19%] [G loss: 1.179920] [time: 47.973610]\n",
      "190 [D loss: 0.686456, acc.: 62.50%] [G loss: 1.003604] [time: 48.194096]\n",
      "191 [D loss: 0.623959, acc.: 67.19%] [G loss: 1.027623] [time: 48.412264]\n",
      "192 [D loss: 0.614425, acc.: 65.62%] [G loss: 0.950663] [time: 48.634642]\n",
      "193 [D loss: 0.625690, acc.: 65.62%] [G loss: 1.038267] [time: 48.854422]\n",
      "194 [D loss: 0.647814, acc.: 65.62%] [G loss: 1.091019] [time: 49.072105]\n",
      "195 [D loss: 0.651699, acc.: 67.19%] [G loss: 0.998419] [time: 49.292008]\n",
      "196 [D loss: 0.670822, acc.: 64.06%] [G loss: 1.082241] [time: 49.514060]\n",
      "197 [D loss: 0.513215, acc.: 82.81%] [G loss: 1.078227] [time: 49.737008]\n",
      "198 [D loss: 0.495002, acc.: 79.69%] [G loss: 1.231940] [time: 49.958731]\n",
      "199 [D loss: 0.617106, acc.: 68.75%] [G loss: 1.166730] [time: 50.176950]\n",
      "200 [D loss: 0.621400, acc.: 70.31%] [G loss: 1.241107] [time: 50.396023]\n",
      "201 [D loss: 0.751614, acc.: 59.38%] [G loss: 1.057087] [time: 50.618391]\n",
      "202 [D loss: 0.626381, acc.: 70.31%] [G loss: 1.090903] [time: 50.836420]\n",
      "203 [D loss: 0.621044, acc.: 65.62%] [G loss: 1.018741] [time: 51.054176]\n",
      "204 [D loss: 0.629552, acc.: 71.88%] [G loss: 1.068075] [time: 51.271613]\n",
      "205 [D loss: 0.565635, acc.: 75.00%] [G loss: 1.145138] [time: 51.489626]\n",
      "206 [D loss: 0.685848, acc.: 62.50%] [G loss: 1.109805] [time: 51.710949]\n",
      "207 [D loss: 0.635950, acc.: 64.06%] [G loss: 1.071244] [time: 51.931240]\n",
      "208 [D loss: 0.654517, acc.: 64.06%] [G loss: 1.041255] [time: 52.149997]\n",
      "209 [D loss: 0.582964, acc.: 75.00%] [G loss: 1.158964] [time: 52.370108]\n",
      "210 [D loss: 0.596333, acc.: 71.88%] [G loss: 1.239477] [time: 52.594396]\n",
      "211 [D loss: 0.635918, acc.: 65.62%] [G loss: 1.238221] [time: 52.816988]\n",
      "212 [D loss: 0.637902, acc.: 67.19%] [G loss: 1.105606] [time: 53.036652]\n",
      "213 [D loss: 0.670493, acc.: 65.62%] [G loss: 1.027275] [time: 53.259635]\n",
      "214 [D loss: 0.580221, acc.: 71.88%] [G loss: 1.131049] [time: 53.477469]\n",
      "215 [D loss: 0.503170, acc.: 82.81%] [G loss: 1.186915] [time: 53.698646]\n",
      "216 [D loss: 0.609783, acc.: 71.88%] [G loss: 1.294646] [time: 53.918130]\n",
      "217 [D loss: 0.545235, acc.: 75.00%] [G loss: 1.242484] [time: 54.137198]\n",
      "218 [D loss: 0.571694, acc.: 68.75%] [G loss: 1.182124] [time: 54.357678]\n",
      "219 [D loss: 0.593402, acc.: 71.88%] [G loss: 1.236427] [time: 54.579538]\n",
      "220 [D loss: 0.640221, acc.: 67.19%] [G loss: 1.159332] [time: 54.799631]\n",
      "221 [D loss: 0.577525, acc.: 65.62%] [G loss: 1.302258] [time: 55.021099]\n",
      "222 [D loss: 0.619527, acc.: 67.19%] [G loss: 1.343392] [time: 55.239077]\n",
      "223 [D loss: 0.636580, acc.: 60.94%] [G loss: 1.119533] [time: 55.459537]\n",
      "224 [D loss: 0.626177, acc.: 68.75%] [G loss: 1.156741] [time: 55.676669]\n",
      "225 [D loss: 0.694215, acc.: 57.81%] [G loss: 1.059297] [time: 55.894949]\n",
      "226 [D loss: 0.616966, acc.: 70.31%] [G loss: 1.049396] [time: 56.115664]\n",
      "227 [D loss: 0.547002, acc.: 75.00%] [G loss: 1.106758] [time: 56.335127]\n",
      "228 [D loss: 0.550195, acc.: 75.00%] [G loss: 1.229001] [time: 56.553437]\n",
      "229 [D loss: 0.665767, acc.: 64.06%] [G loss: 1.291626] [time: 56.775358]\n",
      "230 [D loss: 0.685115, acc.: 57.81%] [G loss: 1.108854] [time: 56.995791]\n",
      "231 [D loss: 0.564656, acc.: 67.19%] [G loss: 1.155743] [time: 57.216667]\n",
      "232 [D loss: 0.586805, acc.: 70.31%] [G loss: 1.209816] [time: 57.436265]\n",
      "233 [D loss: 0.511977, acc.: 79.69%] [G loss: 1.474896] [time: 57.653770]\n",
      "234 [D loss: 0.644738, acc.: 68.75%] [G loss: 1.170573] [time: 57.878347]\n",
      "235 [D loss: 0.572009, acc.: 68.75%] [G loss: 1.259683] [time: 58.097031]\n",
      "236 [D loss: 0.621256, acc.: 60.94%] [G loss: 1.311977] [time: 58.316154]\n",
      "237 [D loss: 0.589507, acc.: 71.88%] [G loss: 1.340046] [time: 58.537255]\n",
      "238 [D loss: 0.548780, acc.: 75.00%] [G loss: 1.479418] [time: 58.752326]\n",
      "239 [D loss: 0.722933, acc.: 57.81%] [G loss: 1.041021] [time: 58.973727]\n",
      "240 [D loss: 0.706653, acc.: 59.38%] [G loss: 1.009926] [time: 59.195628]\n",
      "241 [D loss: 0.636028, acc.: 65.62%] [G loss: 0.966130] [time: 59.414921]\n",
      "242 [D loss: 0.648312, acc.: 64.06%] [G loss: 1.069741] [time: 59.636436]\n",
      "243 [D loss: 0.559914, acc.: 75.00%] [G loss: 0.971377] [time: 59.854025]\n",
      "244 [D loss: 0.524393, acc.: 78.12%] [G loss: 1.108187] [time: 60.072150]\n",
      "245 [D loss: 0.641951, acc.: 71.88%] [G loss: 1.162375] [time: 60.294586]\n",
      "246 [D loss: 0.490456, acc.: 81.25%] [G loss: 1.103858] [time: 60.516915]\n",
      "247 [D loss: 0.599180, acc.: 71.88%] [G loss: 1.209532] [time: 60.742013]\n",
      "248 [D loss: 0.625289, acc.: 67.19%] [G loss: 1.240055] [time: 60.959641]\n",
      "249 [D loss: 0.620915, acc.: 71.88%] [G loss: 1.200847] [time: 61.181997]\n",
      "250 [D loss: 0.520961, acc.: 79.69%] [G loss: 1.137234] [time: 61.398967]\n",
      "251 [D loss: 0.591746, acc.: 70.31%] [G loss: 1.277801] [time: 61.617438]\n",
      "252 [D loss: 0.624281, acc.: 68.75%] [G loss: 1.032377] [time: 61.839776]\n",
      "253 [D loss: 0.611466, acc.: 67.19%] [G loss: 0.911806] [time: 62.061190]\n",
      "254 [D loss: 0.564543, acc.: 68.75%] [G loss: 1.102973] [time: 62.283991]\n",
      "255 [D loss: 0.533282, acc.: 71.88%] [G loss: 1.444657] [time: 62.506385]\n",
      "256 [D loss: 0.631501, acc.: 67.19%] [G loss: 1.402156] [time: 62.725397]\n",
      "257 [D loss: 0.551496, acc.: 70.31%] [G loss: 1.545375] [time: 62.949120]\n",
      "258 [D loss: 0.606161, acc.: 67.19%] [G loss: 1.737235] [time: 63.169222]\n",
      "259 [D loss: 0.490454, acc.: 75.00%] [G loss: 1.714272] [time: 63.394442]\n",
      "260 [D loss: 0.559910, acc.: 76.56%] [G loss: 1.555600] [time: 63.612726]\n",
      "261 [D loss: 0.585068, acc.: 71.88%] [G loss: 1.678573] [time: 63.834965]\n",
      "262 [D loss: 0.666446, acc.: 64.06%] [G loss: 1.609581] [time: 64.053351]\n",
      "263 [D loss: 0.689882, acc.: 65.62%] [G loss: 1.354601] [time: 64.274586]\n",
      "264 [D loss: 0.632130, acc.: 71.88%] [G loss: 1.314490] [time: 64.493917]\n",
      "265 [D loss: 0.675238, acc.: 62.50%] [G loss: 1.073110] [time: 64.714589]\n",
      "266 [D loss: 0.606733, acc.: 70.31%] [G loss: 1.083144] [time: 64.937118]\n",
      "267 [D loss: 0.634233, acc.: 65.62%] [G loss: 1.126600] [time: 65.158333]\n",
      "268 [D loss: 0.613703, acc.: 67.19%] [G loss: 1.113241] [time: 65.379201]\n",
      "269 [D loss: 0.699109, acc.: 60.94%] [G loss: 0.760327] [time: 65.597690]\n",
      "270 [D loss: 0.599314, acc.: 73.44%] [G loss: 0.915424] [time: 65.820565]\n",
      "271 [D loss: 0.540432, acc.: 76.56%] [G loss: 1.187273] [time: 66.046032]\n",
      "272 [D loss: 0.556645, acc.: 73.44%] [G loss: 1.540937] [time: 66.271700]\n",
      "273 [D loss: 0.706619, acc.: 62.50%] [G loss: 1.361681] [time: 66.493109]\n",
      "274 [D loss: 0.562127, acc.: 71.88%] [G loss: 1.415437] [time: 66.716186]\n",
      "275 [D loss: 0.569394, acc.: 71.88%] [G loss: 1.113434] [time: 66.941392]\n",
      "276 [D loss: 0.692227, acc.: 56.25%] [G loss: 0.830201] [time: 67.161731]\n",
      "277 [D loss: 0.661681, acc.: 60.94%] [G loss: 0.791476] [time: 67.385780]\n",
      "278 [D loss: 0.639152, acc.: 64.06%] [G loss: 0.944026] [time: 67.607421]\n",
      "279 [D loss: 0.607251, acc.: 60.94%] [G loss: 1.000805] [time: 67.829857]\n",
      "280 [D loss: 0.597591, acc.: 68.75%] [G loss: 1.273306] [time: 68.052413]\n",
      "281 [D loss: 0.770178, acc.: 40.62%] [G loss: 0.725749] [time: 68.278430]\n",
      "282 [D loss: 0.620337, acc.: 65.62%] [G loss: 0.767369] [time: 68.502833]\n",
      "283 [D loss: 0.631516, acc.: 67.19%] [G loss: 0.852272] [time: 68.723691]\n",
      "284 [D loss: 0.609082, acc.: 67.19%] [G loss: 1.009274] [time: 68.944065]\n",
      "285 [D loss: 0.613805, acc.: 67.19%] [G loss: 1.064934] [time: 69.167038]\n",
      "286 [D loss: 0.618230, acc.: 67.19%] [G loss: 1.124692] [time: 69.387021]\n",
      "287 [D loss: 0.546394, acc.: 70.31%] [G loss: 1.290611] [time: 69.612676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 [D loss: 0.590490, acc.: 67.19%] [G loss: 1.325398] [time: 69.836192]\n",
      "289 [D loss: 0.600388, acc.: 67.19%] [G loss: 1.416505] [time: 70.061983]\n",
      "290 [D loss: 0.613318, acc.: 67.19%] [G loss: 1.348998] [time: 70.282127]\n",
      "291 [D loss: 0.633641, acc.: 67.19%] [G loss: 0.869675] [time: 70.505130]\n",
      "292 [D loss: 0.617417, acc.: 62.50%] [G loss: 0.803590] [time: 70.729576]\n",
      "293 [D loss: 0.526967, acc.: 81.25%] [G loss: 0.981924] [time: 70.956621]\n",
      "294 [D loss: 0.689710, acc.: 57.81%] [G loss: 1.184628] [time: 71.184073]\n",
      "295 [D loss: 0.688711, acc.: 57.81%] [G loss: 1.100962] [time: 71.407768]\n",
      "296 [D loss: 0.582364, acc.: 73.44%] [G loss: 1.212375] [time: 71.630146]\n",
      "297 [D loss: 0.620190, acc.: 64.06%] [G loss: 1.216454] [time: 71.852887]\n",
      "298 [D loss: 0.548070, acc.: 73.44%] [G loss: 1.313273] [time: 72.070159]\n",
      "299 [D loss: 0.598910, acc.: 70.31%] [G loss: 1.477735] [time: 72.296732]\n",
      "300 [D loss: 0.602167, acc.: 70.31%] [G loss: 1.183430] [time: 72.516881]\n",
      "301 [D loss: 0.580516, acc.: 68.75%] [G loss: 1.055745] [time: 72.740610]\n",
      "302 [D loss: 0.631659, acc.: 68.75%] [G loss: 1.308249] [time: 72.966168]\n",
      "303 [D loss: 0.604761, acc.: 65.62%] [G loss: 1.364781] [time: 73.191874]\n",
      "304 [D loss: 0.574662, acc.: 65.62%] [G loss: 1.522630] [time: 73.414298]\n",
      "305 [D loss: 0.608164, acc.: 67.19%] [G loss: 1.587694] [time: 73.643338]\n",
      "306 [D loss: 0.638301, acc.: 59.38%] [G loss: 1.449456] [time: 73.865516]\n",
      "307 [D loss: 0.579192, acc.: 75.00%] [G loss: 1.429061] [time: 74.088647]\n",
      "308 [D loss: 0.580825, acc.: 70.31%] [G loss: 1.340840] [time: 74.311796]\n",
      "309 [D loss: 0.638223, acc.: 65.62%] [G loss: 1.506765] [time: 74.534438]\n",
      "310 [D loss: 0.563383, acc.: 68.75%] [G loss: 1.712962] [time: 74.756430]\n",
      "311 [D loss: 0.523655, acc.: 75.00%] [G loss: 1.243432] [time: 74.978155]\n",
      "312 [D loss: 0.643798, acc.: 59.38%] [G loss: 1.180811] [time: 75.200147]\n",
      "313 [D loss: 0.442403, acc.: 78.12%] [G loss: 1.290741] [time: 75.423390]\n",
      "314 [D loss: 0.510061, acc.: 78.12%] [G loss: 1.311497] [time: 75.644636]\n",
      "315 [D loss: 0.529535, acc.: 71.88%] [G loss: 1.537514] [time: 75.864289]\n",
      "316 [D loss: 0.494315, acc.: 78.12%] [G loss: 1.592088] [time: 76.084356]\n",
      "317 [D loss: 0.562530, acc.: 73.44%] [G loss: 1.551244] [time: 76.307783]\n",
      "318 [D loss: 0.475643, acc.: 76.56%] [G loss: 2.044743] [time: 76.528685]\n",
      "319 [D loss: 0.780200, acc.: 68.75%] [G loss: 1.841435] [time: 76.751128]\n",
      "320 [D loss: 0.707504, acc.: 71.88%] [G loss: 1.763826] [time: 76.972308]\n",
      "321 [D loss: 1.119503, acc.: 59.38%] [G loss: 1.107990] [time: 77.191025]\n",
      "322 [D loss: 0.993745, acc.: 0.00%] [G loss: 0.944509] [time: 77.409894]\n",
      "323 [D loss: 0.774171, acc.: 29.69%] [G loss: 1.079176] [time: 77.627808]\n",
      "324 [D loss: 0.905525, acc.: 50.00%] [G loss: 0.806000] [time: 77.851544]\n",
      "325 [D loss: 0.779542, acc.: 37.50%] [G loss: 0.760017] [time: 78.070235]\n",
      "326 [D loss: 0.757273, acc.: 1.56%] [G loss: 0.763278] [time: 78.290231]\n",
      "327 [D loss: 0.713522, acc.: 32.81%] [G loss: 0.839132] [time: 78.507935]\n",
      "328 [D loss: 0.736073, acc.: 56.25%] [G loss: 0.783801] [time: 78.727381]\n",
      "329 [D loss: 0.741364, acc.: 53.12%] [G loss: 0.772425] [time: 78.946239]\n",
      "330 [D loss: 0.759533, acc.: 37.50%] [G loss: 0.729353] [time: 79.167976]\n",
      "331 [D loss: 0.727136, acc.: 10.94%] [G loss: 0.725426] [time: 79.388349]\n",
      "332 [D loss: 0.721727, acc.: 26.56%] [G loss: 0.731007] [time: 79.606857]\n",
      "333 [D loss: 0.722998, acc.: 46.88%] [G loss: 0.738343] [time: 79.823918]\n",
      "334 [D loss: 0.725004, acc.: 28.12%] [G loss: 0.722117] [time: 80.042562]\n",
      "335 [D loss: 0.711714, acc.: 23.44%] [G loss: 0.732687] [time: 80.263087]\n",
      "336 [D loss: 0.717083, acc.: 18.75%] [G loss: 0.727086] [time: 80.485150]\n",
      "337 [D loss: 0.714796, acc.: 20.31%] [G loss: 0.719404] [time: 80.705454]\n",
      "338 [D loss: 0.707377, acc.: 29.69%] [G loss: 0.722576] [time: 80.932022]\n",
      "339 [D loss: 0.702711, acc.: 37.50%] [G loss: 0.734683] [time: 81.152738]\n",
      "340 [D loss: 0.707993, acc.: 35.94%] [G loss: 0.752849] [time: 81.371130]\n",
      "341 [D loss: 0.681613, acc.: 64.06%] [G loss: 0.774598] [time: 81.593409]\n",
      "342 [D loss: 0.715388, acc.: 53.12%] [G loss: 0.760355] [time: 81.817301]\n",
      "343 [D loss: 0.688087, acc.: 59.38%] [G loss: 0.774051] [time: 82.038076]\n",
      "344 [D loss: 0.710859, acc.: 53.12%] [G loss: 0.786770] [time: 82.255539]\n",
      "345 [D loss: 0.698448, acc.: 57.81%] [G loss: 0.771056] [time: 82.476976]\n",
      "346 [D loss: 0.705135, acc.: 54.69%] [G loss: 0.765078] [time: 82.696882]\n",
      "347 [D loss: 0.714906, acc.: 50.00%] [G loss: 0.743763] [time: 82.914950]\n",
      "348 [D loss: 0.709358, acc.: 45.31%] [G loss: 0.747889] [time: 83.137111]\n",
      "349 [D loss: 0.688081, acc.: 51.56%] [G loss: 0.747765] [time: 83.355647]\n",
      "350 [D loss: 0.700467, acc.: 40.62%] [G loss: 0.758799] [time: 83.576805]\n",
      "351 [D loss: 0.702121, acc.: 48.44%] [G loss: 0.747188] [time: 83.797929]\n",
      "352 [D loss: 0.693248, acc.: 50.00%] [G loss: 0.765411] [time: 84.019191]\n",
      "353 [D loss: 0.687923, acc.: 57.81%] [G loss: 0.762197] [time: 84.241140]\n",
      "354 [D loss: 0.679468, acc.: 60.94%] [G loss: 0.752575] [time: 84.462821]\n",
      "355 [D loss: 0.685049, acc.: 57.81%] [G loss: 0.789819] [time: 84.681422]\n",
      "356 [D loss: 0.700806, acc.: 50.00%] [G loss: 0.789489] [time: 84.903039]\n",
      "357 [D loss: 0.711872, acc.: 50.00%] [G loss: 0.760387] [time: 85.125479]\n",
      "358 [D loss: 0.708046, acc.: 43.75%] [G loss: 0.752363] [time: 85.346448]\n",
      "359 [D loss: 0.690812, acc.: 59.38%] [G loss: 0.757339] [time: 85.567806]\n",
      "360 [D loss: 0.701878, acc.: 43.75%] [G loss: 0.739211] [time: 85.790489]\n",
      "361 [D loss: 0.699587, acc.: 48.44%] [G loss: 0.762040] [time: 86.009299]\n",
      "362 [D loss: 0.705597, acc.: 48.44%] [G loss: 0.752495] [time: 86.229227]\n",
      "363 [D loss: 0.682199, acc.: 56.25%] [G loss: 0.756534] [time: 86.449747]\n",
      "364 [D loss: 0.686503, acc.: 65.62%] [G loss: 0.753132] [time: 86.670542]\n",
      "365 [D loss: 0.675463, acc.: 62.50%] [G loss: 0.786211] [time: 86.892129]\n",
      "366 [D loss: 0.701791, acc.: 56.25%] [G loss: 0.775033] [time: 87.111819]\n",
      "367 [D loss: 0.666795, acc.: 62.50%] [G loss: 0.781856] [time: 87.332190]\n",
      "368 [D loss: 0.669841, acc.: 59.38%] [G loss: 0.825784] [time: 87.552044]\n",
      "369 [D loss: 0.700574, acc.: 57.81%] [G loss: 0.805515] [time: 87.768870]\n",
      "370 [D loss: 0.701438, acc.: 54.69%] [G loss: 0.777156] [time: 87.985471]\n",
      "371 [D loss: 0.699244, acc.: 50.00%] [G loss: 0.765130] [time: 88.205557]\n",
      "372 [D loss: 0.707170, acc.: 51.56%] [G loss: 0.763680] [time: 88.427912]\n",
      "373 [D loss: 0.677477, acc.: 64.06%] [G loss: 0.764432] [time: 88.648483]\n",
      "374 [D loss: 0.723290, acc.: 46.88%] [G loss: 0.751210] [time: 88.868228]\n",
      "375 [D loss: 0.675176, acc.: 56.25%] [G loss: 0.779975] [time: 89.089480]\n",
      "376 [D loss: 0.684056, acc.: 51.56%] [G loss: 0.792551] [time: 89.310677]\n",
      "377 [D loss: 0.690405, acc.: 54.69%] [G loss: 0.798019] [time: 89.531920]\n",
      "378 [D loss: 0.683036, acc.: 46.88%] [G loss: 0.798188] [time: 89.752890]\n",
      "379 [D loss: 0.724782, acc.: 43.75%] [G loss: 0.756194] [time: 89.974725]\n",
      "380 [D loss: 0.699301, acc.: 56.25%] [G loss: 0.746158] [time: 90.196139]\n",
      "381 [D loss: 0.697724, acc.: 56.25%] [G loss: 0.745537] [time: 90.414240]\n",
      "382 [D loss: 0.714630, acc.: 40.62%] [G loss: 0.734506] [time: 90.634257]\n",
      "383 [D loss: 0.705608, acc.: 42.19%] [G loss: 0.735485] [time: 90.852813]\n",
      "384 [D loss: 0.689205, acc.: 51.56%] [G loss: 0.725652] [time: 91.073811]\n",
      "385 [D loss: 0.686065, acc.: 56.25%] [G loss: 0.737603] [time: 91.295959]\n",
      "386 [D loss: 0.706355, acc.: 45.31%] [G loss: 0.743789] [time: 91.515630]\n",
      "387 [D loss: 0.693656, acc.: 48.44%] [G loss: 0.729673] [time: 91.738146]\n",
      "388 [D loss: 0.695876, acc.: 60.94%] [G loss: 0.739329] [time: 91.962332]\n",
      "389 [D loss: 0.688702, acc.: 54.69%] [G loss: 0.753137] [time: 92.183568]\n",
      "390 [D loss: 0.691703, acc.: 54.69%] [G loss: 0.745348] [time: 92.405519]\n",
      "391 [D loss: 0.674802, acc.: 59.38%] [G loss: 0.757781] [time: 92.627310]\n",
      "392 [D loss: 0.704909, acc.: 43.75%] [G loss: 0.749734] [time: 92.847923]\n",
      "393 [D loss: 0.671612, acc.: 60.94%] [G loss: 0.789099] [time: 93.068504]\n",
      "394 [D loss: 0.700343, acc.: 45.31%] [G loss: 0.778990] [time: 93.293409]\n",
      "395 [D loss: 0.680432, acc.: 59.38%] [G loss: 0.796018] [time: 93.516488]\n",
      "396 [D loss: 0.665059, acc.: 59.38%] [G loss: 0.813010] [time: 93.740783]\n",
      "397 [D loss: 0.730320, acc.: 43.75%] [G loss: 0.757377] [time: 93.960568]\n",
      "398 [D loss: 0.687299, acc.: 54.69%] [G loss: 0.755151] [time: 94.182137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 [D loss: 0.695516, acc.: 51.56%] [G loss: 0.772787] [time: 94.404164]\n",
      "400 [D loss: 0.711175, acc.: 53.12%] [G loss: 0.766322] [time: 94.629194]\n",
      "401 [D loss: 0.697481, acc.: 57.81%] [G loss: 0.762312] [time: 94.847689]\n",
      "402 [D loss: 0.705752, acc.: 45.31%] [G loss: 0.754389] [time: 95.070529]\n",
      "403 [D loss: 0.710112, acc.: 42.19%] [G loss: 0.739235] [time: 95.293231]\n",
      "404 [D loss: 0.703693, acc.: 51.56%] [G loss: 0.744321] [time: 95.515237]\n",
      "405 [D loss: 0.688577, acc.: 56.25%] [G loss: 0.738477] [time: 95.738403]\n",
      "406 [D loss: 0.682157, acc.: 60.94%] [G loss: 0.761345] [time: 95.959710]\n",
      "407 [D loss: 0.706599, acc.: 39.06%] [G loss: 0.749514] [time: 96.185251]\n",
      "408 [D loss: 0.719544, acc.: 43.75%] [G loss: 0.729430] [time: 96.409593]\n",
      "409 [D loss: 0.700295, acc.: 46.88%] [G loss: 0.722947] [time: 96.626607]\n",
      "410 [D loss: 0.695680, acc.: 57.81%] [G loss: 0.728070] [time: 96.849337]\n",
      "411 [D loss: 0.703015, acc.: 51.56%] [G loss: 0.708546] [time: 97.071752]\n",
      "412 [D loss: 0.686329, acc.: 57.81%] [G loss: 0.720481] [time: 97.294830]\n",
      "413 [D loss: 0.685155, acc.: 57.81%] [G loss: 0.765877] [time: 97.519545]\n",
      "414 [D loss: 0.695656, acc.: 54.69%] [G loss: 0.741370] [time: 97.744156]\n",
      "415 [D loss: 0.672583, acc.: 67.19%] [G loss: 0.763548] [time: 97.967396]\n",
      "416 [D loss: 0.694082, acc.: 54.69%] [G loss: 0.737341] [time: 98.189425]\n",
      "417 [D loss: 0.713028, acc.: 43.75%] [G loss: 0.761792] [time: 98.407042]\n",
      "418 [D loss: 0.665604, acc.: 57.81%] [G loss: 0.768421] [time: 98.629700]\n",
      "419 [D loss: 0.692250, acc.: 48.44%] [G loss: 0.785947] [time: 98.849991]\n",
      "420 [D loss: 0.666737, acc.: 60.94%] [G loss: 0.756829] [time: 99.073640]\n",
      "421 [D loss: 0.692909, acc.: 62.50%] [G loss: 0.754046] [time: 99.299497]\n",
      "422 [D loss: 0.696901, acc.: 51.56%] [G loss: 0.760121] [time: 99.521008]\n",
      "423 [D loss: 0.682645, acc.: 54.69%] [G loss: 0.759751] [time: 99.745161]\n",
      "424 [D loss: 0.688261, acc.: 60.94%] [G loss: 0.936136] [time: 99.967602]\n",
      "425 [D loss: 0.701335, acc.: 54.69%] [G loss: 0.838708] [time: 100.191150]\n",
      "426 [D loss: 0.721932, acc.: 53.12%] [G loss: 0.777201] [time: 100.416243]\n",
      "427 [D loss: 0.719191, acc.: 46.88%] [G loss: 0.753491] [time: 100.641052]\n",
      "428 [D loss: 0.670329, acc.: 65.62%] [G loss: 0.746336] [time: 100.866405]\n",
      "429 [D loss: 0.664238, acc.: 65.62%] [G loss: 0.780960] [time: 101.090662]\n",
      "430 [D loss: 0.677545, acc.: 62.50%] [G loss: 0.768169] [time: 101.314341]\n",
      "431 [D loss: 0.707582, acc.: 45.31%] [G loss: 0.752347] [time: 101.539462]\n",
      "432 [D loss: 0.689617, acc.: 59.38%] [G loss: 0.746698] [time: 101.761977]\n",
      "433 [D loss: 0.701435, acc.: 46.88%] [G loss: 0.731662] [time: 101.984475]\n",
      "434 [D loss: 0.671646, acc.: 60.94%] [G loss: 0.783560] [time: 102.207040]\n",
      "435 [D loss: 0.688221, acc.: 59.38%] [G loss: 0.755024] [time: 102.428281]\n",
      "436 [D loss: 0.711120, acc.: 45.31%] [G loss: 0.747764] [time: 102.650534]\n",
      "437 [D loss: 0.694689, acc.: 51.56%] [G loss: 0.739360] [time: 102.876229]\n",
      "438 [D loss: 0.714977, acc.: 46.88%] [G loss: 0.720142] [time: 103.099453]\n",
      "439 [D loss: 0.691933, acc.: 57.81%] [G loss: 0.733780] [time: 103.324867]\n",
      "440 [D loss: 0.682202, acc.: 60.94%] [G loss: 0.756414] [time: 103.549494]\n",
      "441 [D loss: 0.700949, acc.: 51.56%] [G loss: 0.723755] [time: 103.773975]\n",
      "442 [D loss: 0.686115, acc.: 60.94%] [G loss: 0.713431] [time: 103.994868]\n",
      "443 [D loss: 0.682872, acc.: 59.38%] [G loss: 0.708041] [time: 104.218330]\n",
      "444 [D loss: 0.687090, acc.: 56.25%] [G loss: 0.751839] [time: 104.441463]\n",
      "445 [D loss: 0.703648, acc.: 54.69%] [G loss: 0.760028] [time: 104.665686]\n",
      "446 [D loss: 0.715619, acc.: 48.44%] [G loss: 0.708369] [time: 104.882780]\n",
      "447 [D loss: 0.675446, acc.: 62.50%] [G loss: 0.715553] [time: 105.104290]\n",
      "448 [D loss: 0.692835, acc.: 56.25%] [G loss: 0.733947] [time: 105.329461]\n",
      "449 [D loss: 0.689742, acc.: 57.81%] [G loss: 0.726135] [time: 105.551695]\n",
      "450 [D loss: 0.713340, acc.: 43.75%] [G loss: 0.704805] [time: 105.775349]\n",
      "451 [D loss: 0.693123, acc.: 51.56%] [G loss: 0.705213] [time: 106.001204]\n",
      "452 [D loss: 0.689171, acc.: 56.25%] [G loss: 0.723159] [time: 106.222022]\n",
      "453 [D loss: 0.681180, acc.: 57.81%] [G loss: 0.719498] [time: 106.443922]\n",
      "454 [D loss: 0.704213, acc.: 53.12%] [G loss: 0.725625] [time: 106.668300]\n",
      "455 [D loss: 0.672696, acc.: 62.50%] [G loss: 0.718664] [time: 106.891173]\n",
      "456 [D loss: 0.697845, acc.: 56.25%] [G loss: 0.751479] [time: 107.115015]\n",
      "457 [D loss: 0.683137, acc.: 56.25%] [G loss: 0.717676] [time: 107.340392]\n",
      "458 [D loss: 0.693123, acc.: 51.56%] [G loss: 0.714502] [time: 107.567067]\n",
      "459 [D loss: 0.689677, acc.: 57.81%] [G loss: 0.719189] [time: 107.788190]\n",
      "460 [D loss: 0.679084, acc.: 64.06%] [G loss: 0.709214] [time: 108.010331]\n",
      "461 [D loss: 0.665251, acc.: 67.19%] [G loss: 0.739166] [time: 108.237221]\n",
      "462 [D loss: 0.699328, acc.: 48.44%] [G loss: 0.709323] [time: 108.458420]\n",
      "463 [D loss: 0.689448, acc.: 56.25%] [G loss: 0.751395] [time: 108.683589]\n",
      "464 [D loss: 0.695996, acc.: 53.12%] [G loss: 0.728685] [time: 108.902768]\n",
      "465 [D loss: 0.686683, acc.: 60.94%] [G loss: 0.709621] [time: 109.124445]\n",
      "466 [D loss: 0.686707, acc.: 56.25%] [G loss: 0.718664] [time: 109.347049]\n",
      "467 [D loss: 0.694656, acc.: 51.56%] [G loss: 0.745990] [time: 109.570819]\n",
      "468 [D loss: 0.677459, acc.: 60.94%] [G loss: 0.783993] [time: 109.793030]\n",
      "469 [D loss: 0.670150, acc.: 67.19%] [G loss: 0.734890] [time: 110.016744]\n",
      "470 [D loss: 0.667044, acc.: 64.06%] [G loss: 0.752183] [time: 110.241879]\n",
      "471 [D loss: 0.660037, acc.: 59.38%] [G loss: 0.761196] [time: 110.465370]\n",
      "472 [D loss: 0.648297, acc.: 64.06%] [G loss: 0.795505] [time: 110.688386]\n",
      "473 [D loss: 0.687167, acc.: 62.50%] [G loss: 0.825296] [time: 110.910968]\n",
      "474 [D loss: 0.726594, acc.: 46.88%] [G loss: 0.775560] [time: 111.134293]\n",
      "475 [D loss: 0.712305, acc.: 45.31%] [G loss: 0.748824] [time: 111.357589]\n",
      "476 [D loss: 0.706587, acc.: 53.12%] [G loss: 0.733964] [time: 111.583385]\n",
      "477 [D loss: 0.726106, acc.: 37.50%] [G loss: 0.730467] [time: 111.804820]\n",
      "478 [D loss: 0.698635, acc.: 54.69%] [G loss: 0.727586] [time: 112.027318]\n",
      "479 [D loss: 0.699316, acc.: 45.31%] [G loss: 0.711500] [time: 112.250870]\n",
      "480 [D loss: 0.692141, acc.: 54.69%] [G loss: 0.708818] [time: 112.474180]\n",
      "481 [D loss: 0.695462, acc.: 57.81%] [G loss: 0.705398] [time: 112.699195]\n",
      "482 [D loss: 0.706795, acc.: 46.88%] [G loss: 0.698419] [time: 112.923507]\n",
      "483 [D loss: 0.710980, acc.: 37.50%] [G loss: 0.703676] [time: 113.149286]\n",
      "484 [D loss: 0.707072, acc.: 40.62%] [G loss: 0.704223] [time: 113.374722]\n",
      "485 [D loss: 0.686887, acc.: 65.62%] [G loss: 0.706776] [time: 113.598486]\n",
      "486 [D loss: 0.709149, acc.: 48.44%] [G loss: 0.700488] [time: 113.821567]\n",
      "487 [D loss: 0.712129, acc.: 32.81%] [G loss: 0.701794] [time: 114.045480]\n",
      "488 [D loss: 0.697604, acc.: 54.69%] [G loss: 0.702576] [time: 114.272560]\n",
      "489 [D loss: 0.691354, acc.: 56.25%] [G loss: 0.706607] [time: 114.495463]\n",
      "490 [D loss: 0.696329, acc.: 53.12%] [G loss: 0.705753] [time: 114.721504]\n",
      "491 [D loss: 0.687575, acc.: 60.94%] [G loss: 0.709194] [time: 114.942423]\n",
      "492 [D loss: 0.708067, acc.: 45.31%] [G loss: 0.698008] [time: 115.167237]\n",
      "493 [D loss: 0.690414, acc.: 56.25%] [G loss: 0.708971] [time: 115.392255]\n",
      "494 [D loss: 0.693986, acc.: 53.12%] [G loss: 0.710024] [time: 115.614353]\n",
      "495 [D loss: 0.690686, acc.: 56.25%] [G loss: 0.712036] [time: 115.839629]\n",
      "496 [D loss: 0.702599, acc.: 48.44%] [G loss: 0.700878] [time: 116.059846]\n",
      "497 [D loss: 0.701306, acc.: 51.56%] [G loss: 0.708110] [time: 116.285932]\n",
      "498 [D loss: 0.686489, acc.: 65.62%] [G loss: 0.701839] [time: 116.508663]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8488d496a352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m   \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-8488d496a352>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# Generate a midi file in a periodic basis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.layers import Input, Dense, Reshape, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "\n",
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files \"\"\"\n",
    "    notes = []\n",
    "    i = 0\n",
    "    for file in glob.glob(\"pokemon_data/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "        i = i + 1\n",
    "        if i > 5: break\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "            \n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return notes\n",
    "\n",
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 250\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # Reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # Normalize input between -1 and 1\n",
    "    network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)\n",
    "\n",
    "def generate_notes(model, network_input, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = numpy.random.randint(0, len(network_input)-1)\n",
    "    \n",
    "    # Get pitch names and store in a dictionary\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = numpy.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern = numpy.append(pattern,index)\n",
    "        #pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output\n",
    "  \n",
    "def create_midi(prediction_output, filename):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for item in prediction_output:\n",
    "        pattern = item[0]\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, rows):\n",
    "        self.seq_length = rows\n",
    "        self.seq_shape = (self.seq_length, 1)\n",
    "        self.latent_dim = 1000\n",
    "        self.disc_loss = []\n",
    "        self.gen_loss =[]\n",
    "        \n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates note sequences\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        generated_seq = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(generated_seq)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
    "        model.add(Bidirectional(CuDNNLSTM(512)))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        seq = Input(shape=self.seq_shape)\n",
    "        validity = model(seq)\n",
    "\n",
    "        return Model(seq, validity)\n",
    "      \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.seq_shape))\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        seq = model(noise)\n",
    "\n",
    "        return Model(noise, seq)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        print('[train]: Starting...')\n",
    "        time_start = time.time()\n",
    "        # Load and convert the data\n",
    "        notes = get_notes()\n",
    "        n_vocab = len(set(notes))\n",
    "        X_train, y_train = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Insert saving; The generator image is saved every 500 steps\n",
    "        save_interval = 500\n",
    "\n",
    "        # Training the model\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Training the discriminator\n",
    "            # Select a random batch of note sequences\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_seqs = X_train[idx]\n",
    "\n",
    "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
    "            #noise = (noise-242)/242\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new note sequences\n",
    "            gen_seqs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            #  Training the Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, real)\n",
    "\n",
    "\n",
    "\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "\n",
    "                # Save the generator model in a periodic basis\n",
    "                model_name = './pokemon_models/pokemon_' + str(epoch)\n",
    "                self.generator.save(model_name + \".h5\")\n",
    "\n",
    "                # Generate a midi file in a periodic basis\n",
    "                self.generate(notes, epoch)\n",
    "\n",
    "\n",
    "            # Print the progress and save into loss lists\n",
    "            if epoch % sample_interval == 0:\n",
    "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [time: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss, time.time() - time_start))\n",
    "              self.disc_loss.append(d_loss[0])\n",
    "              self.gen_loss.append(g_loss)\n",
    "        \n",
    "        self.generate(notes, epoch)\n",
    "        self.plot_loss()\n",
    "\n",
    "\n",
    "        time_end = time.time()\n",
    "        print('[train]: Ended. Time elapsed: {}'.format(time_end - time_start))\n",
    "        \n",
    "    def generate(self, input_notes):\n",
    "        # Get pitch names and store in a dictionary\n",
    "        notes = input_notes\n",
    "        pitchnames = sorted(set(item for item in notes))\n",
    "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "        \n",
    "        # Use random noise to generate sequences\n",
    "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
    "        predictions = self.generator.predict(noise)\n",
    "        \n",
    "        pred_notes = [x*242+242 for x in predictions[0]]\n",
    "        pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
    "        \n",
    "\n",
    "        file_name = './pokemon_output/pokemon_' + str(epoch)\n",
    "        create_midi(pred_notes, file_name)\n",
    "        time_end = time.time()\n",
    "        print('[generate midi]: Ended. Time elapsed: {}'.format(time_end - time_start))\n",
    "\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.disc_loss, c='red')\n",
    "        plt.plot(self.gen_loss, c='blue')\n",
    "        plt.title(\"GAN Loss per Epoch\")\n",
    "        plt.legend(['Discriminator', 'Generator'])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig('./pokemon_output/GAN_Loss_per_Epoch_final.png', transparent=True)\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  gan = GAN(rows=250)    \n",
    "  gan.train(epochs=500, batch_size=32, sample_interval=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coe197] *",
   "language": "python",
   "name": "conda-env-coe197-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
