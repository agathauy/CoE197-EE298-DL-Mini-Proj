{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_5 (CuDNNLSTM)     (None, 250, 512)          1054720   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 1024)              4202496   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,913,601\n",
      "Trainable params: 5,913,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               256256    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 250)               256250    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 250, 1)            0         \n",
      "=================================================================\n",
      "Total params: 1,176,570\n",
      "Trainable params: 1,172,986\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "[train]: Starting...\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Approaching Champion Cynthia.mid\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Battle Champion.mid\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Battle Team Galactic Admin.mid\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Battle Rival.mid\n",
      "Parsing pokemon_data/Pokemon DiamondPearlPlatinum - Azelf Mesprit and Uxie Battle.mid\n",
      "0 [D loss: 0.694983, acc.: 20.31%] [G loss: 0.688050] [time: 7.308870]\n",
      "1 [D loss: 0.684868, acc.: 51.56%] [G loss: 0.685873] [time: 7.538251]\n",
      "2 [D loss: 0.671623, acc.: 57.81%] [G loss: 0.687073] [time: 7.760346]\n",
      "3 [D loss: 0.645416, acc.: 62.50%] [G loss: 0.686787] [time: 7.983242]\n",
      "4 [D loss: 0.601668, acc.: 68.75%] [G loss: 0.786653] [time: 8.206698]\n",
      "5 [D loss: 1.009249, acc.: 68.75%] [G loss: 0.857417] [time: 8.430573]\n",
      "6 [D loss: 0.448151, acc.: 84.38%] [G loss: 0.897108] [time: 8.654382]\n",
      "7 [D loss: 0.483025, acc.: 87.50%] [G loss: 1.179946] [time: 8.876252]\n",
      "8 [D loss: 0.514849, acc.: 84.38%] [G loss: 1.342089] [time: 9.098855]\n",
      "9 [D loss: 0.310365, acc.: 90.62%] [G loss: 4.230110] [time: 9.321032]\n",
      "10 [D loss: 0.303578, acc.: 93.75%] [G loss: 4.048731] [time: 9.544507]\n",
      "11 [D loss: 0.363913, acc.: 90.62%] [G loss: 2.468408] [time: 9.762637]\n",
      "12 [D loss: 0.164429, acc.: 98.44%] [G loss: 3.877499] [time: 9.985959]\n",
      "13 [D loss: 0.347672, acc.: 90.62%] [G loss: 2.038428] [time: 10.207865]\n",
      "14 [D loss: 0.308508, acc.: 85.94%] [G loss: 2.225334] [time: 10.431384]\n",
      "15 [D loss: 0.230086, acc.: 93.75%] [G loss: 2.615779] [time: 10.653144]\n",
      "16 [D loss: 0.316414, acc.: 89.06%] [G loss: 3.004081] [time: 10.876207]\n",
      "17 [D loss: 0.240611, acc.: 92.19%] [G loss: 2.715264] [time: 11.095818]\n",
      "18 [D loss: 0.314553, acc.: 92.19%] [G loss: 4.213135] [time: 11.317204]\n",
      "19 [D loss: 0.846715, acc.: 75.00%] [G loss: 1.143891] [time: 11.538898]\n",
      "20 [D loss: 0.484015, acc.: 84.38%] [G loss: 1.186944] [time: 11.761637]\n",
      "21 [D loss: 0.489015, acc.: 75.00%] [G loss: 1.421644] [time: 11.981551]\n",
      "22 [D loss: 0.357591, acc.: 89.06%] [G loss: 2.693684] [time: 12.203910]\n",
      "23 [D loss: 0.276316, acc.: 92.19%] [G loss: 2.599275] [time: 12.423112]\n",
      "24 [D loss: 0.276249, acc.: 84.38%] [G loss: 2.440737] [time: 12.648991]\n",
      "25 [D loss: 0.369515, acc.: 85.94%] [G loss: 2.263974] [time: 12.869334]\n",
      "26 [D loss: 0.755435, acc.: 73.44%] [G loss: 2.228986] [time: 13.090701]\n",
      "27 [D loss: 0.463747, acc.: 79.69%] [G loss: 1.780888] [time: 13.314183]\n",
      "28 [D loss: 0.617278, acc.: 70.31%] [G loss: 1.600137] [time: 13.532672]\n",
      "29 [D loss: 0.550744, acc.: 68.75%] [G loss: 1.682615] [time: 13.751448]\n",
      "30 [D loss: 0.805065, acc.: 68.75%] [G loss: 2.081306] [time: 13.975284]\n",
      "31 [D loss: 0.603778, acc.: 71.88%] [G loss: 1.209593] [time: 14.197937]\n",
      "32 [D loss: 0.518179, acc.: 76.56%] [G loss: 1.213258] [time: 14.416942]\n",
      "33 [D loss: 0.502930, acc.: 78.12%] [G loss: 2.444838] [time: 14.637661]\n",
      "34 [D loss: 0.533763, acc.: 79.69%] [G loss: 1.835338] [time: 14.856441]\n",
      "35 [D loss: 0.751185, acc.: 73.44%] [G loss: 1.105110] [time: 15.075033]\n",
      "36 [D loss: 0.763399, acc.: 62.50%] [G loss: 0.922005] [time: 15.294462]\n",
      "37 [D loss: 0.637904, acc.: 67.19%] [G loss: 0.952817] [time: 15.516255]\n",
      "38 [D loss: 0.620185, acc.: 68.75%] [G loss: 0.951655] [time: 15.737049]\n",
      "39 [D loss: 0.581770, acc.: 68.75%] [G loss: 1.080973] [time: 15.960121]\n",
      "40 [D loss: 0.633966, acc.: 65.62%] [G loss: 1.044278] [time: 16.184493]\n",
      "41 [D loss: 0.539518, acc.: 73.44%] [G loss: 1.127346] [time: 16.405399]\n",
      "42 [D loss: 0.563123, acc.: 71.88%] [G loss: 1.132640] [time: 16.625952]\n",
      "43 [D loss: 0.586058, acc.: 68.75%] [G loss: 1.071534] [time: 16.847119]\n",
      "44 [D loss: 0.527443, acc.: 73.44%] [G loss: 1.141675] [time: 17.072644]\n",
      "45 [D loss: 0.499138, acc.: 75.00%] [G loss: 1.044187] [time: 17.292978]\n",
      "46 [D loss: 0.501711, acc.: 75.00%] [G loss: 1.175803] [time: 17.513736]\n",
      "47 [D loss: 0.532892, acc.: 70.31%] [G loss: 1.179928] [time: 17.737832]\n",
      "48 [D loss: 0.558954, acc.: 68.75%] [G loss: 1.127678] [time: 17.958905]\n",
      "49 [D loss: 0.619152, acc.: 65.62%] [G loss: 1.100756] [time: 18.178955]\n",
      "50 [D loss: 0.547911, acc.: 70.31%] [G loss: 1.125576] [time: 18.397323]\n",
      "51 [D loss: 0.596439, acc.: 65.62%] [G loss: 1.118577] [time: 18.618599]\n",
      "52 [D loss: 0.657726, acc.: 62.50%] [G loss: 1.006058] [time: 18.836622]\n",
      "53 [D loss: 0.616088, acc.: 64.06%] [G loss: 0.998421] [time: 19.058070]\n",
      "54 [D loss: 0.621303, acc.: 62.50%] [G loss: 1.003141] [time: 19.277249]\n",
      "55 [D loss: 0.609184, acc.: 62.50%] [G loss: 0.991753] [time: 19.495683]\n",
      "56 [D loss: 0.651155, acc.: 59.38%] [G loss: 0.975809] [time: 19.713332]\n",
      "57 [D loss: 0.615140, acc.: 62.50%] [G loss: 0.927191] [time: 19.934630]\n",
      "58 [D loss: 0.529688, acc.: 68.75%] [G loss: 1.043733] [time: 20.156238]\n",
      "59 [D loss: 0.554260, acc.: 65.62%] [G loss: 1.075250] [time: 20.373080]\n",
      "60 [D loss: 0.628447, acc.: 60.94%] [G loss: 1.070364] [time: 20.594165]\n",
      "61 [D loss: 0.612880, acc.: 60.94%] [G loss: 1.020192] [time: 20.813059]\n",
      "62 [D loss: 0.553699, acc.: 65.62%] [G loss: 1.080847] [time: 21.033535]\n",
      "63 [D loss: 0.617537, acc.: 62.50%] [G loss: 1.077214] [time: 21.255020]\n",
      "64 [D loss: 0.530463, acc.: 71.88%] [G loss: 1.101573] [time: 21.474960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 [D loss: 0.588406, acc.: 67.19%] [G loss: 1.129282] [time: 21.696610]\n",
      "66 [D loss: 0.591102, acc.: 64.06%] [G loss: 1.154762] [time: 21.917524]\n",
      "67 [D loss: 0.611527, acc.: 67.19%] [G loss: 1.157943] [time: 22.139057]\n",
      "68 [D loss: 0.576031, acc.: 73.44%] [G loss: 1.215367] [time: 22.358922]\n",
      "69 [D loss: 0.753453, acc.: 54.69%] [G loss: 0.953474] [time: 22.579995]\n",
      "70 [D loss: 0.551716, acc.: 70.31%] [G loss: 0.995333] [time: 22.799789]\n",
      "71 [D loss: 0.532776, acc.: 71.88%] [G loss: 1.041493] [time: 23.019585]\n",
      "72 [D loss: 0.604348, acc.: 68.75%] [G loss: 1.059740] [time: 23.238485]\n",
      "73 [D loss: 0.514217, acc.: 73.44%] [G loss: 1.079107] [time: 23.458284]\n",
      "74 [D loss: 0.573355, acc.: 67.19%] [G loss: 1.110654] [time: 23.679294]\n",
      "75 [D loss: 0.635243, acc.: 65.62%] [G loss: 1.090424] [time: 23.899553]\n",
      "76 [D loss: 0.467001, acc.: 76.56%] [G loss: 1.182709] [time: 24.119615]\n",
      "77 [D loss: 0.572107, acc.: 68.75%] [G loss: 1.163296] [time: 24.343065]\n",
      "78 [D loss: 0.505901, acc.: 75.00%] [G loss: 1.207759] [time: 24.561867]\n",
      "79 [D loss: 0.435231, acc.: 82.81%] [G loss: 1.339556] [time: 24.782800]\n",
      "80 [D loss: 0.559704, acc.: 70.31%] [G loss: 1.294595] [time: 25.000882]\n",
      "81 [D loss: 0.621590, acc.: 71.88%] [G loss: 1.185945] [time: 25.220314]\n",
      "82 [D loss: 0.579020, acc.: 71.88%] [G loss: 1.220590] [time: 25.440906]\n",
      "83 [D loss: 0.693083, acc.: 64.06%] [G loss: 1.121787] [time: 25.661320]\n",
      "84 [D loss: 0.536922, acc.: 76.56%] [G loss: 1.228032] [time: 25.881243]\n",
      "85 [D loss: 0.504420, acc.: 79.69%] [G loss: 1.671805] [time: 26.095324]\n",
      "86 [D loss: 0.647173, acc.: 71.88%] [G loss: 1.171593] [time: 26.314070]\n",
      "87 [D loss: 0.587434, acc.: 68.75%] [G loss: 1.214568] [time: 26.535002]\n",
      "88 [D loss: 0.592671, acc.: 68.75%] [G loss: 1.268126] [time: 26.757663]\n",
      "89 [D loss: 0.541401, acc.: 79.69%] [G loss: 1.313641] [time: 26.982348]\n",
      "90 [D loss: 0.454792, acc.: 82.81%] [G loss: 1.589032] [time: 27.199482]\n",
      "91 [D loss: 0.489380, acc.: 79.69%] [G loss: 1.457402] [time: 27.418227]\n",
      "92 [D loss: 0.483371, acc.: 78.12%] [G loss: 1.474514] [time: 27.637005]\n",
      "93 [D loss: 0.450204, acc.: 79.69%] [G loss: 1.454447] [time: 27.860425]\n",
      "94 [D loss: 0.655339, acc.: 68.75%] [G loss: 1.203429] [time: 28.080054]\n",
      "95 [D loss: 0.546485, acc.: 73.44%] [G loss: 1.147421] [time: 28.300595]\n",
      "96 [D loss: 0.568234, acc.: 68.75%] [G loss: 1.211269] [time: 28.516804]\n",
      "97 [D loss: 0.565161, acc.: 65.62%] [G loss: 1.273691] [time: 28.736348]\n",
      "98 [D loss: 0.520326, acc.: 64.06%] [G loss: 1.185356] [time: 28.957513]\n",
      "99 [D loss: 0.438271, acc.: 75.00%] [G loss: 1.390707] [time: 29.179857]\n",
      "100 [D loss: 0.585600, acc.: 60.94%] [G loss: 1.314950] [time: 29.399160]\n",
      "101 [D loss: 0.478840, acc.: 78.12%] [G loss: 1.316701] [time: 29.619570]\n",
      "102 [D loss: 0.573893, acc.: 70.31%] [G loss: 1.339590] [time: 29.839794]\n",
      "103 [D loss: 0.537250, acc.: 67.19%] [G loss: 1.237025] [time: 30.062852]\n",
      "104 [D loss: 0.473418, acc.: 81.25%] [G loss: 1.325912] [time: 30.286735]\n",
      "105 [D loss: 0.660852, acc.: 78.12%] [G loss: 1.303255] [time: 30.505902]\n",
      "106 [D loss: 0.453947, acc.: 78.12%] [G loss: 1.492433] [time: 30.722821]\n",
      "107 [D loss: 0.643171, acc.: 64.06%] [G loss: 1.422918] [time: 30.939392]\n",
      "108 [D loss: 0.592225, acc.: 64.06%] [G loss: 1.248737] [time: 31.160406]\n",
      "109 [D loss: 0.447641, acc.: 81.25%] [G loss: 1.550053] [time: 31.377872]\n",
      "110 [D loss: 0.702674, acc.: 59.38%] [G loss: 1.128431] [time: 31.598091]\n",
      "111 [D loss: 0.541549, acc.: 70.31%] [G loss: 1.143598] [time: 31.815878]\n",
      "112 [D loss: 0.497066, acc.: 81.25%] [G loss: 1.341370] [time: 32.037579]\n",
      "113 [D loss: 0.524269, acc.: 73.44%] [G loss: 2.051546] [time: 32.254863]\n",
      "114 [D loss: 0.617184, acc.: 67.19%] [G loss: 1.007897] [time: 32.476360]\n",
      "115 [D loss: 0.526899, acc.: 70.31%] [G loss: 1.178571] [time: 32.696433]\n",
      "116 [D loss: 0.610547, acc.: 67.19%] [G loss: 1.327202] [time: 32.917841]\n",
      "117 [D loss: 0.571815, acc.: 71.88%] [G loss: 1.361617] [time: 33.139263]\n",
      "118 [D loss: 0.552899, acc.: 71.88%] [G loss: 1.403279] [time: 33.360436]\n",
      "119 [D loss: 0.621553, acc.: 62.50%] [G loss: 1.062363] [time: 33.582191]\n",
      "120 [D loss: 0.500738, acc.: 71.88%] [G loss: 1.170441] [time: 33.805001]\n",
      "121 [D loss: 0.492042, acc.: 78.12%] [G loss: 1.409405] [time: 34.029660]\n",
      "122 [D loss: 0.529947, acc.: 82.81%] [G loss: 1.243111] [time: 34.251237]\n",
      "123 [D loss: 0.583361, acc.: 67.19%] [G loss: 1.359329] [time: 34.472932]\n",
      "124 [D loss: 0.620685, acc.: 57.81%] [G loss: 1.253912] [time: 34.691187]\n",
      "125 [D loss: 0.626990, acc.: 67.19%] [G loss: 1.041719] [time: 34.911497]\n",
      "126 [D loss: 0.609301, acc.: 65.62%] [G loss: 1.059180] [time: 35.132948]\n",
      "127 [D loss: 0.542900, acc.: 71.88%] [G loss: 1.137731] [time: 35.354250]\n",
      "128 [D loss: 0.468974, acc.: 79.69%] [G loss: 1.613228] [time: 35.575528]\n",
      "129 [D loss: 0.766148, acc.: 60.94%] [G loss: 0.902157] [time: 35.796541]\n",
      "130 [D loss: 0.584876, acc.: 68.75%] [G loss: 0.968100] [time: 36.014691]\n",
      "131 [D loss: 0.480682, acc.: 79.69%] [G loss: 1.045133] [time: 36.236643]\n",
      "132 [D loss: 0.494729, acc.: 75.00%] [G loss: 1.289420] [time: 36.454821]\n",
      "133 [D loss: 0.495118, acc.: 81.25%] [G loss: 1.332575] [time: 36.677854]\n",
      "134 [D loss: 0.567028, acc.: 71.88%] [G loss: 1.155967] [time: 36.898730]\n",
      "135 [D loss: 0.599939, acc.: 70.31%] [G loss: 1.094776] [time: 37.119372]\n",
      "136 [D loss: 0.593301, acc.: 60.94%] [G loss: 1.082905] [time: 37.339445]\n",
      "137 [D loss: 0.615292, acc.: 68.75%] [G loss: 1.044954] [time: 37.558861]\n",
      "138 [D loss: 0.594127, acc.: 70.31%] [G loss: 0.992362] [time: 37.781855]\n",
      "139 [D loss: 0.582413, acc.: 73.44%] [G loss: 1.116885] [time: 38.003561]\n",
      "140 [D loss: 0.464477, acc.: 76.56%] [G loss: 1.294488] [time: 38.225191]\n",
      "141 [D loss: 0.515369, acc.: 71.88%] [G loss: 1.279035] [time: 38.447773]\n",
      "142 [D loss: 0.647336, acc.: 57.81%] [G loss: 1.484601] [time: 38.668758]\n",
      "143 [D loss: 0.661682, acc.: 60.94%] [G loss: 1.135563] [time: 38.887411]\n",
      "144 [D loss: 0.616955, acc.: 64.06%] [G loss: 1.139080] [time: 39.106997]\n",
      "145 [D loss: 0.573226, acc.: 71.88%] [G loss: 1.172639] [time: 39.327215]\n",
      "146 [D loss: 0.604561, acc.: 65.62%] [G loss: 1.166216] [time: 39.546336]\n",
      "147 [D loss: 0.582153, acc.: 68.75%] [G loss: 1.188872] [time: 39.766812]\n",
      "148 [D loss: 0.631582, acc.: 65.62%] [G loss: 1.095198] [time: 39.988100]\n",
      "149 [D loss: 0.637305, acc.: 62.50%] [G loss: 1.007139] [time: 40.208844]\n",
      "150 [D loss: 0.632806, acc.: 59.38%] [G loss: 1.142995] [time: 40.430954]\n",
      "151 [D loss: 0.511745, acc.: 75.00%] [G loss: 1.510385] [time: 40.650773]\n",
      "152 [D loss: 0.804162, acc.: 46.88%] [G loss: 0.817769] [time: 40.872139]\n",
      "153 [D loss: 0.656236, acc.: 62.50%] [G loss: 0.922389] [time: 41.094394]\n",
      "154 [D loss: 0.560858, acc.: 70.31%] [G loss: 1.174235] [time: 41.315184]\n",
      "155 [D loss: 0.677993, acc.: 54.69%] [G loss: 0.758972] [time: 41.535065]\n",
      "156 [D loss: 0.613898, acc.: 64.06%] [G loss: 0.838696] [time: 41.751417]\n",
      "157 [D loss: 0.638703, acc.: 60.94%] [G loss: 1.178148] [time: 41.973543]\n",
      "158 [D loss: 0.582208, acc.: 65.62%] [G loss: 1.401956] [time: 42.189837]\n",
      "159 [D loss: 0.611075, acc.: 67.19%] [G loss: 0.774281] [time: 42.413649]\n",
      "160 [D loss: 0.562913, acc.: 76.56%] [G loss: 0.783923] [time: 42.634035]\n",
      "161 [D loss: 0.610525, acc.: 70.31%] [G loss: 0.850768] [time: 42.856337]\n",
      "162 [D loss: 0.705387, acc.: 59.38%] [G loss: 0.801390] [time: 43.075407]\n",
      "163 [D loss: 0.609467, acc.: 64.06%] [G loss: 0.939272] [time: 43.295599]\n",
      "164 [D loss: 0.627937, acc.: 65.62%] [G loss: 1.008994] [time: 43.518140]\n",
      "165 [D loss: 0.579987, acc.: 67.19%] [G loss: 1.043692] [time: 43.738374]\n",
      "166 [D loss: 0.682753, acc.: 62.50%] [G loss: 1.128480] [time: 43.961265]\n",
      "167 [D loss: 0.688656, acc.: 53.12%] [G loss: 0.994627] [time: 44.179157]\n",
      "168 [D loss: 0.597876, acc.: 67.19%] [G loss: 1.069465] [time: 44.399698]\n",
      "169 [D loss: 0.586564, acc.: 67.19%] [G loss: 1.324684] [time: 44.620168]\n",
      "170 [D loss: 0.556117, acc.: 71.88%] [G loss: 1.330753] [time: 44.842476]\n",
      "171 [D loss: 0.584709, acc.: 60.94%] [G loss: 1.200926] [time: 45.064240]\n",
      "172 [D loss: 0.553557, acc.: 70.31%] [G loss: 2.031694] [time: 45.285046]\n",
      "173 [D loss: 0.683944, acc.: 64.06%] [G loss: 1.361239] [time: 45.504864]\n",
      "174 [D loss: 0.746308, acc.: 57.81%] [G loss: 1.124487] [time: 45.727092]\n",
      "175 [D loss: 0.566254, acc.: 65.62%] [G loss: 1.306495] [time: 45.948873]\n",
      "176 [D loss: 0.675455, acc.: 60.94%] [G loss: 0.877707] [time: 46.172002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 [D loss: 0.647377, acc.: 59.38%] [G loss: 0.930281] [time: 46.394727]\n",
      "178 [D loss: 0.596699, acc.: 70.31%] [G loss: 0.916744] [time: 46.616474]\n",
      "179 [D loss: 0.675691, acc.: 57.81%] [G loss: 1.012301] [time: 46.838968]\n",
      "180 [D loss: 0.581510, acc.: 68.75%] [G loss: 1.008848] [time: 47.058334]\n",
      "181 [D loss: 0.480417, acc.: 84.38%] [G loss: 1.213946] [time: 47.282761]\n",
      "182 [D loss: 0.655147, acc.: 67.19%] [G loss: 0.893947] [time: 47.502207]\n",
      "183 [D loss: 0.642858, acc.: 71.88%] [G loss: 0.997156] [time: 47.723006]\n",
      "184 [D loss: 0.653281, acc.: 59.38%] [G loss: 0.940645] [time: 47.940915]\n",
      "185 [D loss: 0.674656, acc.: 57.81%] [G loss: 0.938088] [time: 48.161052]\n",
      "186 [D loss: 0.588765, acc.: 73.44%] [G loss: 0.906611] [time: 48.381045]\n",
      "187 [D loss: 0.624234, acc.: 62.50%] [G loss: 0.931746] [time: 48.599789]\n",
      "188 [D loss: 0.671341, acc.: 57.81%] [G loss: 1.028337] [time: 48.824251]\n",
      "189 [D loss: 0.583902, acc.: 73.44%] [G loss: 1.016665] [time: 49.045132]\n",
      "190 [D loss: 0.661987, acc.: 56.25%] [G loss: 1.064411] [time: 49.262850]\n",
      "191 [D loss: 0.599748, acc.: 65.62%] [G loss: 1.160253] [time: 49.484298]\n",
      "192 [D loss: 0.641663, acc.: 64.06%] [G loss: 1.200353] [time: 49.705243]\n",
      "193 [D loss: 0.543290, acc.: 73.44%] [G loss: 1.433563] [time: 49.927417]\n",
      "194 [D loss: 0.496185, acc.: 76.56%] [G loss: 1.430230] [time: 50.146951]\n",
      "195 [D loss: 0.628574, acc.: 62.50%] [G loss: 1.062858] [time: 50.366035]\n",
      "196 [D loss: 0.531285, acc.: 70.31%] [G loss: 1.362882] [time: 50.583660]\n",
      "197 [D loss: 0.570747, acc.: 64.06%] [G loss: 1.517915] [time: 50.804813]\n",
      "198 [D loss: 0.567972, acc.: 73.44%] [G loss: 1.428818] [time: 51.023991]\n",
      "199 [D loss: 0.605580, acc.: 64.06%] [G loss: 1.519688] [time: 51.244846]\n",
      "200 [D loss: 0.734546, acc.: 56.25%] [G loss: 1.231249] [time: 51.466195]\n",
      "201 [D loss: 0.599363, acc.: 67.19%] [G loss: 1.081886] [time: 51.686421]\n",
      "202 [D loss: 0.684423, acc.: 53.12%] [G loss: 1.213769] [time: 51.907459]\n",
      "203 [D loss: 0.604805, acc.: 62.50%] [G loss: 1.244436] [time: 52.128066]\n",
      "204 [D loss: 0.641839, acc.: 56.25%] [G loss: 1.184125] [time: 52.351468]\n",
      "205 [D loss: 0.622036, acc.: 73.44%] [G loss: 1.187805] [time: 52.569550]\n",
      "206 [D loss: 0.566445, acc.: 71.88%] [G loss: 1.205601] [time: 52.790215]\n",
      "207 [D loss: 0.520810, acc.: 75.00%] [G loss: 1.596947] [time: 53.011911]\n",
      "208 [D loss: 0.566267, acc.: 68.75%] [G loss: 1.613482] [time: 53.230857]\n",
      "209 [D loss: 0.532609, acc.: 71.88%] [G loss: 1.568026] [time: 53.448785]\n",
      "210 [D loss: 0.581604, acc.: 67.19%] [G loss: 1.846955] [time: 53.670366]\n",
      "211 [D loss: 0.726912, acc.: 70.31%] [G loss: 1.369453] [time: 53.893120]\n",
      "212 [D loss: 0.751707, acc.: 59.38%] [G loss: 1.044320] [time: 54.115127]\n",
      "213 [D loss: 0.682409, acc.: 57.81%] [G loss: 1.108195] [time: 54.333947]\n",
      "214 [D loss: 0.644108, acc.: 65.62%] [G loss: 1.050632] [time: 54.559460]\n",
      "215 [D loss: 0.632042, acc.: 60.94%] [G loss: 1.024002] [time: 54.780239]\n",
      "216 [D loss: 0.737501, acc.: 43.75%] [G loss: 0.921277] [time: 55.001577]\n",
      "217 [D loss: 0.628953, acc.: 54.69%] [G loss: 1.121150] [time: 55.221027]\n",
      "218 [D loss: 0.615653, acc.: 59.38%] [G loss: 1.485613] [time: 55.442531]\n",
      "219 [D loss: 0.543519, acc.: 73.44%] [G loss: 1.573698] [time: 55.664496]\n",
      "220 [D loss: 0.685137, acc.: 57.81%] [G loss: 1.152199] [time: 55.883279]\n",
      "221 [D loss: 0.596032, acc.: 70.31%] [G loss: 0.960237] [time: 56.106576]\n",
      "222 [D loss: 0.693171, acc.: 60.94%] [G loss: 0.853565] [time: 56.327907]\n",
      "223 [D loss: 0.628686, acc.: 57.81%] [G loss: 1.152968] [time: 56.549926]\n",
      "224 [D loss: 0.689573, acc.: 57.81%] [G loss: 1.120789] [time: 56.770519]\n",
      "225 [D loss: 0.689208, acc.: 59.38%] [G loss: 1.700935] [time: 56.991966]\n",
      "226 [D loss: 0.772505, acc.: 45.31%] [G loss: 1.372949] [time: 57.214361]\n",
      "227 [D loss: 0.796496, acc.: 68.75%] [G loss: 0.933657] [time: 57.432663]\n",
      "228 [D loss: 0.671556, acc.: 64.06%] [G loss: 1.111755] [time: 57.656667]\n",
      "229 [D loss: 0.869355, acc.: 39.06%] [G loss: 0.683558] [time: 57.876186]\n",
      "230 [D loss: 0.691098, acc.: 54.69%] [G loss: 0.685561] [time: 58.099856]\n",
      "231 [D loss: 0.711810, acc.: 39.06%] [G loss: 0.687032] [time: 58.321226]\n",
      "232 [D loss: 0.700980, acc.: 43.75%] [G loss: 0.671938] [time: 58.541835]\n",
      "233 [D loss: 0.698238, acc.: 50.00%] [G loss: 0.686459] [time: 58.761649]\n",
      "234 [D loss: 0.703529, acc.: 43.75%] [G loss: 0.687852] [time: 58.983005]\n",
      "235 [D loss: 0.702346, acc.: 48.44%] [G loss: 0.677521] [time: 59.204803]\n",
      "236 [D loss: 0.702799, acc.: 46.88%] [G loss: 0.689612] [time: 59.429081]\n",
      "237 [D loss: 0.687598, acc.: 60.94%] [G loss: 0.684907] [time: 59.651484]\n",
      "238 [D loss: 0.691823, acc.: 53.12%] [G loss: 0.691253] [time: 59.873151]\n",
      "239 [D loss: 0.698963, acc.: 42.19%] [G loss: 0.694218] [time: 60.094884]\n",
      "240 [D loss: 0.698239, acc.: 51.56%] [G loss: 0.692163] [time: 60.314220]\n",
      "241 [D loss: 0.697411, acc.: 48.44%] [G loss: 0.690164] [time: 60.535794]\n",
      "242 [D loss: 0.696019, acc.: 50.00%] [G loss: 0.699514] [time: 60.753154]\n",
      "243 [D loss: 0.687439, acc.: 51.56%] [G loss: 0.708797] [time: 60.972830]\n",
      "244 [D loss: 0.689540, acc.: 53.12%] [G loss: 0.702341] [time: 61.195045]\n",
      "245 [D loss: 0.681365, acc.: 65.62%] [G loss: 0.698913] [time: 61.414778]\n",
      "246 [D loss: 0.687711, acc.: 59.38%] [G loss: 0.696699] [time: 61.634723]\n",
      "247 [D loss: 0.689901, acc.: 50.00%] [G loss: 0.708527] [time: 61.851386]\n",
      "248 [D loss: 0.678126, acc.: 65.62%] [G loss: 0.701364] [time: 62.075766]\n",
      "249 [D loss: 0.695670, acc.: 50.00%] [G loss: 0.697950] [time: 62.297673]\n",
      "250 [D loss: 0.688699, acc.: 56.25%] [G loss: 0.694532] [time: 62.517841]\n",
      "251 [D loss: 0.690021, acc.: 53.12%] [G loss: 0.706796] [time: 62.738937]\n",
      "252 [D loss: 0.663843, acc.: 60.94%] [G loss: 0.701119] [time: 62.959186]\n",
      "253 [D loss: 0.680821, acc.: 54.69%] [G loss: 0.713910] [time: 63.179259]\n",
      "254 [D loss: 0.691596, acc.: 48.44%] [G loss: 0.710803] [time: 63.401183]\n",
      "255 [D loss: 0.688576, acc.: 56.25%] [G loss: 0.722141] [time: 63.621629]\n",
      "256 [D loss: 0.672396, acc.: 57.81%] [G loss: 0.727296] [time: 63.846114]\n",
      "257 [D loss: 0.692755, acc.: 53.12%] [G loss: 0.715996] [time: 64.069255]\n",
      "258 [D loss: 0.691250, acc.: 56.25%] [G loss: 0.717392] [time: 64.290237]\n",
      "259 [D loss: 0.683531, acc.: 59.38%] [G loss: 0.711454] [time: 64.512992]\n",
      "260 [D loss: 0.674155, acc.: 56.25%] [G loss: 0.738198] [time: 64.738550]\n",
      "261 [D loss: 0.669078, acc.: 59.38%] [G loss: 0.747590] [time: 64.961644]\n",
      "262 [D loss: 0.657564, acc.: 67.19%] [G loss: 0.752582] [time: 65.184533]\n",
      "263 [D loss: 0.693409, acc.: 46.88%] [G loss: 0.777963] [time: 65.405578]\n",
      "264 [D loss: 0.712393, acc.: 48.44%] [G loss: 0.787521] [time: 65.626248]\n",
      "265 [D loss: 0.668494, acc.: 60.94%] [G loss: 0.854134] [time: 65.846644]\n",
      "266 [D loss: 0.749714, acc.: 53.12%] [G loss: 0.771112] [time: 66.066654]\n",
      "267 [D loss: 0.726675, acc.: 51.56%] [G loss: 0.733454] [time: 66.289321]\n",
      "268 [D loss: 0.685575, acc.: 62.50%] [G loss: 0.802474] [time: 66.509937]\n",
      "269 [D loss: 0.676572, acc.: 62.50%] [G loss: 0.768839] [time: 66.729425]\n",
      "270 [D loss: 0.641812, acc.: 65.62%] [G loss: 1.019861] [time: 66.954150]\n",
      "271 [D loss: 0.791194, acc.: 34.38%] [G loss: 0.658826] [time: 67.173082]\n",
      "272 [D loss: 0.686103, acc.: 54.69%] [G loss: 0.666653] [time: 67.392713]\n",
      "273 [D loss: 0.698024, acc.: 51.56%] [G loss: 0.666562] [time: 67.611824]\n",
      "274 [D loss: 0.692225, acc.: 51.56%] [G loss: 0.678019] [time: 67.832187]\n",
      "275 [D loss: 0.699498, acc.: 50.00%] [G loss: 0.713032] [time: 68.052601]\n",
      "276 [D loss: 0.689181, acc.: 54.69%] [G loss: 0.694412] [time: 68.276896]\n",
      "277 [D loss: 0.704399, acc.: 48.44%] [G loss: 0.705149] [time: 68.500090]\n",
      "278 [D loss: 0.679256, acc.: 60.94%] [G loss: 0.700198] [time: 68.723863]\n",
      "279 [D loss: 0.693642, acc.: 57.81%] [G loss: 0.690487] [time: 68.948864]\n",
      "280 [D loss: 0.681670, acc.: 59.38%] [G loss: 0.686350] [time: 69.170905]\n",
      "281 [D loss: 0.671284, acc.: 68.75%] [G loss: 0.694653] [time: 69.388911]\n",
      "282 [D loss: 0.699304, acc.: 51.56%] [G loss: 0.757594] [time: 69.610272]\n",
      "283 [D loss: 0.674865, acc.: 64.06%] [G loss: 0.758085] [time: 69.831780]\n",
      "284 [D loss: 0.686714, acc.: 60.94%] [G loss: 0.803922] [time: 70.053430]\n",
      "285 [D loss: 0.655991, acc.: 57.81%] [G loss: 0.832084] [time: 70.273159]\n",
      "286 [D loss: 0.627387, acc.: 56.25%] [G loss: 1.270535] [time: 70.495495]\n",
      "287 [D loss: 0.849925, acc.: 45.31%] [G loss: 0.658381] [time: 70.717553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 [D loss: 0.713296, acc.: 57.81%] [G loss: 0.800153] [time: 70.937344]\n",
      "289 [D loss: 0.674052, acc.: 64.06%] [G loss: 0.970022] [time: 71.160956]\n",
      "290 [D loss: 0.618635, acc.: 64.06%] [G loss: 1.874920] [time: 71.380380]\n",
      "291 [D loss: 0.786964, acc.: 62.50%] [G loss: 1.279358] [time: 71.601375]\n",
      "292 [D loss: 0.980229, acc.: 34.38%] [G loss: 0.851823] [time: 71.827323]\n",
      "293 [D loss: 0.665257, acc.: 56.25%] [G loss: 1.012057] [time: 72.051673]\n",
      "294 [D loss: 0.639245, acc.: 59.38%] [G loss: 1.337519] [time: 72.276234]\n",
      "295 [D loss: 0.712777, acc.: 60.94%] [G loss: 1.024288] [time: 72.496944]\n",
      "296 [D loss: 0.631691, acc.: 65.62%] [G loss: 1.071108] [time: 72.721997]\n",
      "297 [D loss: 0.677220, acc.: 62.50%] [G loss: 0.968323] [time: 72.944337]\n",
      "298 [D loss: 0.779609, acc.: 56.25%] [G loss: 0.851553] [time: 73.165634]\n",
      "299 [D loss: 0.680684, acc.: 60.94%] [G loss: 0.840111] [time: 73.386739]\n",
      "300 [D loss: 0.643054, acc.: 65.62%] [G loss: 0.963316] [time: 73.608302]\n",
      "301 [D loss: 0.676593, acc.: 60.94%] [G loss: 0.944285] [time: 73.828414]\n",
      "302 [D loss: 0.723051, acc.: 56.25%] [G loss: 0.888029] [time: 74.053280]\n",
      "303 [D loss: 0.662062, acc.: 64.06%] [G loss: 0.910941] [time: 74.275171]\n",
      "304 [D loss: 0.677547, acc.: 60.94%] [G loss: 0.685600] [time: 74.498106]\n",
      "305 [D loss: 0.691551, acc.: 51.56%] [G loss: 0.713403] [time: 74.719072]\n",
      "306 [D loss: 0.660870, acc.: 62.50%] [G loss: 0.809107] [time: 74.941942]\n",
      "307 [D loss: 0.670146, acc.: 54.69%] [G loss: 0.831887] [time: 75.162746]\n",
      "308 [D loss: 0.639445, acc.: 60.94%] [G loss: 0.892469] [time: 75.386873]\n",
      "309 [D loss: 0.670454, acc.: 56.25%] [G loss: 0.665026] [time: 75.608775]\n",
      "310 [D loss: 0.665505, acc.: 56.25%] [G loss: 0.816248] [time: 75.829762]\n",
      "311 [D loss: 0.667873, acc.: 57.81%] [G loss: 0.963865] [time: 76.050553]\n",
      "312 [D loss: 0.593110, acc.: 68.75%] [G loss: 1.035378] [time: 76.270195]\n",
      "313 [D loss: 0.712581, acc.: 57.81%] [G loss: 1.149449] [time: 76.497767]\n",
      "314 [D loss: 0.717019, acc.: 62.50%] [G loss: 1.031080] [time: 76.723368]\n",
      "315 [D loss: 0.610301, acc.: 67.19%] [G loss: 1.034155] [time: 76.942884]\n",
      "316 [D loss: 0.694588, acc.: 56.25%] [G loss: 1.043732] [time: 77.169274]\n",
      "317 [D loss: 0.661430, acc.: 60.94%] [G loss: 1.003938] [time: 77.390571]\n",
      "318 [D loss: 0.695183, acc.: 54.69%] [G loss: 0.914078] [time: 77.612988]\n",
      "319 [D loss: 0.663293, acc.: 53.12%] [G loss: 0.995197] [time: 77.833911]\n",
      "320 [D loss: 0.718591, acc.: 51.56%] [G loss: 0.875519] [time: 78.056346]\n",
      "321 [D loss: 0.610244, acc.: 73.44%] [G loss: 0.962927] [time: 78.277426]\n",
      "322 [D loss: 0.611392, acc.: 67.19%] [G loss: 1.074828] [time: 78.503373]\n",
      "323 [D loss: 0.707998, acc.: 59.38%] [G loss: 0.956852] [time: 78.727243]\n",
      "324 [D loss: 0.672848, acc.: 62.50%] [G loss: 0.964781] [time: 78.949473]\n",
      "325 [D loss: 0.694573, acc.: 56.25%] [G loss: 0.917431] [time: 79.173744]\n",
      "326 [D loss: 0.676514, acc.: 57.81%] [G loss: 0.977741] [time: 79.398401]\n",
      "327 [D loss: 0.669948, acc.: 70.31%] [G loss: 0.918932] [time: 79.622719]\n",
      "328 [D loss: 0.707560, acc.: 40.62%] [G loss: 0.927891] [time: 79.845426]\n",
      "329 [D loss: 0.694885, acc.: 60.94%] [G loss: 0.854207] [time: 80.067552]\n",
      "330 [D loss: 0.656664, acc.: 64.06%] [G loss: 0.880256] [time: 80.289650]\n",
      "331 [D loss: 0.597524, acc.: 71.88%] [G loss: 1.088249] [time: 80.508399]\n",
      "332 [D loss: 0.601814, acc.: 75.00%] [G loss: 1.026019] [time: 80.731112]\n",
      "333 [D loss: 0.713774, acc.: 53.12%] [G loss: 0.721766] [time: 80.950440]\n",
      "334 [D loss: 0.664475, acc.: 57.81%] [G loss: 0.732021] [time: 81.173951]\n",
      "335 [D loss: 0.676343, acc.: 57.81%] [G loss: 0.782615] [time: 81.392822]\n",
      "336 [D loss: 0.675132, acc.: 51.56%] [G loss: 0.712595] [time: 81.611374]\n",
      "337 [D loss: 0.662298, acc.: 57.81%] [G loss: 0.814718] [time: 81.834181]\n",
      "338 [D loss: 0.659948, acc.: 59.38%] [G loss: 0.781541] [time: 82.059418]\n",
      "339 [D loss: 0.659950, acc.: 59.38%] [G loss: 0.801152] [time: 82.283722]\n",
      "340 [D loss: 0.653442, acc.: 64.06%] [G loss: 0.851087] [time: 82.506193]\n",
      "341 [D loss: 0.668694, acc.: 50.00%] [G loss: 0.885204] [time: 82.728244]\n",
      "342 [D loss: 0.691529, acc.: 48.44%] [G loss: 0.934637] [time: 82.953580]\n",
      "343 [D loss: 0.668397, acc.: 50.00%] [G loss: 0.893422] [time: 83.179084]\n",
      "344 [D loss: 0.642744, acc.: 56.25%] [G loss: 0.950327] [time: 83.404562]\n",
      "345 [D loss: 0.636150, acc.: 56.25%] [G loss: 1.063988] [time: 83.627675]\n",
      "346 [D loss: 0.618213, acc.: 53.12%] [G loss: 1.065791] [time: 83.848333]\n",
      "347 [D loss: 0.630820, acc.: 45.31%] [G loss: 1.096471] [time: 84.072456]\n",
      "348 [D loss: 0.639208, acc.: 56.25%] [G loss: 1.221587] [time: 84.294502]\n",
      "349 [D loss: 0.671060, acc.: 56.25%] [G loss: 1.216392] [time: 84.519723]\n",
      "350 [D loss: 0.703381, acc.: 54.69%] [G loss: 1.063948] [time: 84.740612]\n",
      "351 [D loss: 0.717505, acc.: 56.25%] [G loss: 0.874474] [time: 84.964473]\n",
      "352 [D loss: 0.644713, acc.: 64.06%] [G loss: 0.764146] [time: 85.192899]\n",
      "353 [D loss: 0.666917, acc.: 51.56%] [G loss: 0.852432] [time: 85.419909]\n",
      "354 [D loss: 0.664378, acc.: 54.69%] [G loss: 0.819868] [time: 85.641786]\n",
      "355 [D loss: 0.708968, acc.: 50.00%] [G loss: 0.899542] [time: 85.863996]\n",
      "356 [D loss: 0.671281, acc.: 54.69%] [G loss: 1.003016] [time: 86.083499]\n",
      "357 [D loss: 0.628667, acc.: 65.62%] [G loss: 0.956626] [time: 86.305394]\n",
      "358 [D loss: 0.639962, acc.: 60.94%] [G loss: 0.983197] [time: 86.526783]\n",
      "359 [D loss: 0.625938, acc.: 64.06%] [G loss: 0.957931] [time: 86.752668]\n",
      "360 [D loss: 0.631210, acc.: 68.75%] [G loss: 1.053304] [time: 86.977820]\n",
      "361 [D loss: 0.732404, acc.: 48.44%] [G loss: 0.913679] [time: 87.203132]\n",
      "362 [D loss: 0.620039, acc.: 59.38%] [G loss: 0.853630] [time: 87.427027]\n",
      "363 [D loss: 0.638313, acc.: 62.50%] [G loss: 1.350103] [time: 87.654124]\n",
      "364 [D loss: 0.800315, acc.: 59.38%] [G loss: 1.137221] [time: 87.876446]\n",
      "365 [D loss: 0.782669, acc.: 57.81%] [G loss: 0.842543] [time: 88.104210]\n",
      "366 [D loss: 0.738495, acc.: 40.62%] [G loss: 0.844828] [time: 88.325253]\n",
      "367 [D loss: 0.759998, acc.: 34.38%] [G loss: 0.767105] [time: 88.549879]\n",
      "368 [D loss: 0.806329, acc.: 48.44%] [G loss: 0.818444] [time: 88.775554]\n",
      "369 [D loss: 0.700607, acc.: 57.81%] [G loss: 0.853087] [time: 89.003197]\n",
      "370 [D loss: 0.754526, acc.: 48.44%] [G loss: 0.815876] [time: 89.227116]\n",
      "371 [D loss: 0.723987, acc.: 50.00%] [G loss: 0.778655] [time: 89.449681]\n",
      "372 [D loss: 0.713391, acc.: 50.00%] [G loss: 0.777043] [time: 89.674655]\n",
      "373 [D loss: 0.732560, acc.: 51.56%] [G loss: 0.763736] [time: 89.898909]\n",
      "374 [D loss: 0.744850, acc.: 43.75%] [G loss: 0.742466] [time: 90.122277]\n",
      "375 [D loss: 0.693743, acc.: 51.56%] [G loss: 0.750483] [time: 90.345427]\n",
      "376 [D loss: 0.712349, acc.: 53.12%] [G loss: 0.752350] [time: 90.571022]\n",
      "377 [D loss: 0.684170, acc.: 62.50%] [G loss: 0.789898] [time: 90.794467]\n",
      "378 [D loss: 0.694281, acc.: 48.44%] [G loss: 0.766411] [time: 91.018489]\n",
      "379 [D loss: 0.729289, acc.: 46.88%] [G loss: 0.753185] [time: 91.243146]\n",
      "380 [D loss: 0.709692, acc.: 53.12%] [G loss: 0.747308] [time: 91.466419]\n",
      "381 [D loss: 0.706391, acc.: 54.69%] [G loss: 0.755680] [time: 91.695345]\n",
      "382 [D loss: 0.693995, acc.: 39.06%] [G loss: 0.757361] [time: 91.919537]\n",
      "383 [D loss: 0.683465, acc.: 42.19%] [G loss: 0.737177] [time: 92.145445]\n",
      "384 [D loss: 0.725483, acc.: 51.56%] [G loss: 0.751702] [time: 92.368415]\n",
      "385 [D loss: 0.699396, acc.: 56.25%] [G loss: 0.758689] [time: 92.593107]\n",
      "386 [D loss: 0.724573, acc.: 43.75%] [G loss: 0.753497] [time: 92.820875]\n",
      "387 [D loss: 0.720081, acc.: 40.62%] [G loss: 0.714088] [time: 93.046567]\n",
      "388 [D loss: 0.692694, acc.: 54.69%] [G loss: 0.716693] [time: 93.269235]\n",
      "389 [D loss: 0.717068, acc.: 43.75%] [G loss: 0.766134] [time: 93.493909]\n",
      "390 [D loss: 0.709868, acc.: 48.44%] [G loss: 0.743127] [time: 93.720538]\n",
      "391 [D loss: 0.696745, acc.: 53.12%] [G loss: 0.737452] [time: 93.944678]\n",
      "392 [D loss: 0.739063, acc.: 46.88%] [G loss: 0.702482] [time: 94.169725]\n",
      "393 [D loss: 0.709012, acc.: 39.06%] [G loss: 0.701006] [time: 94.395652]\n",
      "394 [D loss: 0.703577, acc.: 37.50%] [G loss: 0.698877] [time: 94.621856]\n",
      "395 [D loss: 0.705045, acc.: 37.50%] [G loss: 0.680098] [time: 94.846691]\n",
      "396 [D loss: 0.698081, acc.: 46.88%] [G loss: 0.696247] [time: 95.068896]\n",
      "397 [D loss: 0.702233, acc.: 50.00%] [G loss: 0.694356] [time: 95.292379]\n",
      "398 [D loss: 0.701293, acc.: 40.62%] [G loss: 0.696759] [time: 95.515768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 [D loss: 0.699683, acc.: 40.62%] [G loss: 0.685457] [time: 95.739490]\n",
      "400 [D loss: 0.709090, acc.: 35.94%] [G loss: 0.701397] [time: 95.965020]\n",
      "401 [D loss: 0.696132, acc.: 40.62%] [G loss: 0.713244] [time: 96.187557]\n",
      "402 [D loss: 0.700898, acc.: 39.06%] [G loss: 0.718854] [time: 96.412963]\n",
      "403 [D loss: 0.693426, acc.: 59.38%] [G loss: 0.717721] [time: 96.637223]\n",
      "404 [D loss: 0.688287, acc.: 56.25%] [G loss: 0.715210] [time: 96.863914]\n",
      "405 [D loss: 0.691197, acc.: 57.81%] [G loss: 0.723174] [time: 97.088331]\n",
      "406 [D loss: 0.694799, acc.: 54.69%] [G loss: 0.720600] [time: 97.309517]\n",
      "407 [D loss: 0.686852, acc.: 60.94%] [G loss: 0.704952] [time: 97.533551]\n",
      "408 [D loss: 0.705849, acc.: 46.88%] [G loss: 0.714021] [time: 97.756751]\n",
      "409 [D loss: 0.702242, acc.: 46.88%] [G loss: 0.732238] [time: 97.980379]\n",
      "410 [D loss: 0.715968, acc.: 40.62%] [G loss: 0.690886] [time: 98.201562]\n",
      "411 [D loss: 0.697564, acc.: 48.44%] [G loss: 0.695042] [time: 98.427196]\n",
      "412 [D loss: 0.707399, acc.: 35.94%] [G loss: 0.682044] [time: 98.653382]\n",
      "413 [D loss: 0.698624, acc.: 40.62%] [G loss: 0.695941] [time: 98.878254]\n",
      "414 [D loss: 0.698259, acc.: 32.81%] [G loss: 0.673682] [time: 99.102418]\n",
      "415 [D loss: 0.693987, acc.: 59.38%] [G loss: 0.675962] [time: 99.328231]\n",
      "416 [D loss: 0.686144, acc.: 65.62%] [G loss: 0.688269] [time: 99.550354]\n",
      "417 [D loss: 0.689181, acc.: 57.81%] [G loss: 0.697299] [time: 99.777936]\n",
      "418 [D loss: 0.673937, acc.: 62.50%] [G loss: 0.680661] [time: 100.000965]\n",
      "419 [D loss: 0.658068, acc.: 68.75%] [G loss: 0.695903] [time: 100.223230]\n",
      "420 [D loss: 0.685069, acc.: 62.50%] [G loss: 0.750851] [time: 100.447183]\n",
      "421 [D loss: 0.699281, acc.: 48.44%] [G loss: 0.730368] [time: 100.671306]\n",
      "422 [D loss: 0.677245, acc.: 57.81%] [G loss: 0.718844] [time: 100.894858]\n",
      "423 [D loss: 0.675197, acc.: 62.50%] [G loss: 0.766950] [time: 101.121996]\n",
      "424 [D loss: 0.677015, acc.: 56.25%] [G loss: 0.886573] [time: 101.348883]\n",
      "425 [D loss: 0.674362, acc.: 60.94%] [G loss: 0.793190] [time: 101.573626]\n",
      "426 [D loss: 0.650576, acc.: 64.06%] [G loss: 0.775829] [time: 101.796984]\n",
      "427 [D loss: 0.677875, acc.: 62.50%] [G loss: 0.809880] [time: 102.019698]\n",
      "428 [D loss: 0.625466, acc.: 70.31%] [G loss: 0.832222] [time: 102.244807]\n",
      "429 [D loss: 0.675695, acc.: 56.25%] [G loss: 0.830670] [time: 102.471197]\n",
      "430 [D loss: 0.638892, acc.: 67.19%] [G loss: 0.933851] [time: 102.695887]\n",
      "431 [D loss: 0.623938, acc.: 71.88%] [G loss: 0.988114] [time: 102.920346]\n",
      "432 [D loss: 0.746470, acc.: 48.44%] [G loss: 0.814197] [time: 103.144784]\n",
      "433 [D loss: 0.683446, acc.: 60.94%] [G loss: 0.800746] [time: 103.371727]\n",
      "434 [D loss: 0.652574, acc.: 62.50%] [G loss: 0.756016] [time: 103.597620]\n",
      "435 [D loss: 0.677434, acc.: 60.94%] [G loss: 0.804003] [time: 103.820963]\n",
      "436 [D loss: 0.753513, acc.: 50.00%] [G loss: 0.815799] [time: 104.046605]\n",
      "437 [D loss: 0.652123, acc.: 64.06%] [G loss: 0.797502] [time: 104.268659]\n",
      "438 [D loss: 0.659921, acc.: 59.38%] [G loss: 0.801583] [time: 104.494941]\n",
      "439 [D loss: 0.694161, acc.: 54.69%] [G loss: 0.786079] [time: 104.719561]\n",
      "440 [D loss: 0.688656, acc.: 57.81%] [G loss: 0.794486] [time: 104.942309]\n",
      "441 [D loss: 0.688553, acc.: 59.38%] [G loss: 0.751721] [time: 105.165884]\n",
      "442 [D loss: 0.652319, acc.: 67.19%] [G loss: 0.842013] [time: 105.390318]\n",
      "443 [D loss: 0.650756, acc.: 62.50%] [G loss: 0.828823] [time: 105.616127]\n",
      "444 [D loss: 0.626087, acc.: 68.75%] [G loss: 0.945604] [time: 105.841322]\n",
      "445 [D loss: 0.635435, acc.: 62.50%] [G loss: 0.938449] [time: 106.067791]\n",
      "446 [D loss: 0.679340, acc.: 53.12%] [G loss: 1.002175] [time: 106.294248]\n",
      "447 [D loss: 0.643178, acc.: 60.94%] [G loss: 1.045506] [time: 106.521137]\n",
      "448 [D loss: 0.674768, acc.: 54.69%] [G loss: 1.061200] [time: 106.745661]\n",
      "449 [D loss: 0.674534, acc.: 56.25%] [G loss: 0.942688] [time: 106.967484]\n",
      "450 [D loss: 0.686628, acc.: 51.56%] [G loss: 0.934760] [time: 107.189969]\n",
      "451 [D loss: 0.697557, acc.: 54.69%] [G loss: 0.833043] [time: 107.417695]\n",
      "452 [D loss: 0.681917, acc.: 57.81%] [G loss: 0.811467] [time: 107.644647]\n",
      "453 [D loss: 0.658051, acc.: 60.94%] [G loss: 0.800421] [time: 107.867691]\n",
      "454 [D loss: 0.691806, acc.: 51.56%] [G loss: 0.841634] [time: 108.091110]\n",
      "455 [D loss: 0.667070, acc.: 67.19%] [G loss: 0.864106] [time: 108.313439]\n",
      "456 [D loss: 0.668165, acc.: 54.69%] [G loss: 0.844973] [time: 108.534069]\n",
      "457 [D loss: 0.665252, acc.: 53.12%] [G loss: 0.854819] [time: 108.755409]\n",
      "458 [D loss: 0.675259, acc.: 53.12%] [G loss: 0.817261] [time: 108.979438]\n",
      "459 [D loss: 0.669355, acc.: 56.25%] [G loss: 0.838617] [time: 109.200591]\n",
      "460 [D loss: 0.667696, acc.: 56.25%] [G loss: 0.936297] [time: 109.423295]\n",
      "461 [D loss: 0.615820, acc.: 65.62%] [G loss: 0.933008] [time: 109.645136]\n",
      "462 [D loss: 0.684317, acc.: 57.81%] [G loss: 0.938691] [time: 109.864344]\n",
      "463 [D loss: 0.690539, acc.: 60.94%] [G loss: 0.853380] [time: 110.084799]\n",
      "464 [D loss: 0.696760, acc.: 45.31%] [G loss: 0.842502] [time: 110.308824]\n",
      "465 [D loss: 0.661610, acc.: 59.38%] [G loss: 0.800948] [time: 110.532691]\n",
      "466 [D loss: 0.662781, acc.: 56.25%] [G loss: 0.861300] [time: 110.757070]\n",
      "467 [D loss: 0.629544, acc.: 64.06%] [G loss: 0.869548] [time: 110.981628]\n",
      "468 [D loss: 0.673274, acc.: 57.81%] [G loss: 0.953914] [time: 111.208555]\n",
      "469 [D loss: 0.581537, acc.: 76.56%] [G loss: 0.925896] [time: 111.430974]\n",
      "470 [D loss: 0.696330, acc.: 59.38%] [G loss: 0.928496] [time: 111.655556]\n",
      "471 [D loss: 0.708700, acc.: 56.25%] [G loss: 0.866257] [time: 111.880025]\n",
      "472 [D loss: 0.659373, acc.: 62.50%] [G loss: 0.860347] [time: 112.104143]\n",
      "473 [D loss: 0.701090, acc.: 54.69%] [G loss: 0.803192] [time: 112.330282]\n",
      "474 [D loss: 0.659274, acc.: 62.50%] [G loss: 0.768218] [time: 112.553440]\n",
      "475 [D loss: 0.657802, acc.: 67.19%] [G loss: 0.694636] [time: 112.778483]\n",
      "476 [D loss: 0.713725, acc.: 51.56%] [G loss: 0.642432] [time: 113.002818]\n",
      "477 [D loss: 0.685013, acc.: 54.69%] [G loss: 0.772394] [time: 113.229248]\n",
      "478 [D loss: 0.600019, acc.: 71.88%] [G loss: 0.912149] [time: 113.456036]\n",
      "479 [D loss: 0.615318, acc.: 67.19%] [G loss: 0.882894] [time: 113.681315]\n",
      "480 [D loss: 0.637505, acc.: 64.06%] [G loss: 0.940120] [time: 113.901850]\n",
      "481 [D loss: 0.654104, acc.: 62.50%] [G loss: 1.094617] [time: 114.126270]\n",
      "482 [D loss: 0.706958, acc.: 56.25%] [G loss: 0.762524] [time: 114.350389]\n",
      "483 [D loss: 0.601708, acc.: 71.88%] [G loss: 0.731265] [time: 114.575004]\n",
      "484 [D loss: 0.653322, acc.: 62.50%] [G loss: 0.814321] [time: 114.805168]\n",
      "485 [D loss: 0.635289, acc.: 64.06%] [G loss: 1.199264] [time: 115.030900]\n",
      "486 [D loss: 0.627614, acc.: 71.88%] [G loss: 1.171063] [time: 115.261077]\n",
      "487 [D loss: 0.776582, acc.: 60.94%] [G loss: 1.059215] [time: 115.486886]\n",
      "488 [D loss: 0.628172, acc.: 68.75%] [G loss: 0.968483] [time: 115.710602]\n",
      "489 [D loss: 0.654939, acc.: 64.06%] [G loss: 0.945530] [time: 115.934353]\n",
      "490 [D loss: 0.660554, acc.: 65.62%] [G loss: 1.004133] [time: 116.163046]\n",
      "491 [D loss: 0.640656, acc.: 65.62%] [G loss: 1.052935] [time: 116.386917]\n",
      "492 [D loss: 0.689773, acc.: 54.69%] [G loss: 0.871277] [time: 116.609897]\n",
      "493 [D loss: 0.635586, acc.: 68.75%] [G loss: 0.876366] [time: 116.834222]\n",
      "494 [D loss: 0.657854, acc.: 64.06%] [G loss: 0.843104] [time: 117.062008]\n",
      "495 [D loss: 0.692192, acc.: 60.94%] [G loss: 0.798207] [time: 117.286150]\n",
      "496 [D loss: 0.653777, acc.: 62.50%] [G loss: 0.816003] [time: 117.513036]\n",
      "497 [D loss: 0.674044, acc.: 57.81%] [G loss: 0.956740] [time: 117.741132]\n",
      "498 [D loss: 0.669774, acc.: 59.38%] [G loss: 0.852519] [time: 117.966358]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "409",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2ad10a760299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m   \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2ad10a760299>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# Generate a midi file in a periodic basis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ad10a760299>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_notes, epoch)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mpred_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m242\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m242\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mpred_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_note\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_notes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ad10a760299>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mpred_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m242\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m242\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mpred_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_note\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_notes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 409"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.layers import Input, Dense, Reshape, Dropout, CuDNNLSTM, Bidirectional\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "\n",
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files \"\"\"\n",
    "    notes = []\n",
    "    i = 0\n",
    "    for file in glob.glob(\"pokemon_data/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "        i = i + 1\n",
    "        if i > 5: break\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "            \n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return notes\n",
    "\n",
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 250\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # Reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # Normalize input between -1 and 1\n",
    "    network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)\n",
    "\n",
    "def generate_notes(model, network_input, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = numpy.random.randint(0, len(network_input)-1)\n",
    "    \n",
    "    # Get pitch names and store in a dictionary\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = numpy.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern = numpy.append(pattern,index)\n",
    "        #pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output\n",
    "  \n",
    "def create_midi(prediction_output, filename):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for item in prediction_output:\n",
    "        pattern = item[0]\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, rows):\n",
    "        self.seq_length = rows\n",
    "        self.seq_shape = (self.seq_length, 1)\n",
    "        self.latent_dim = 1000\n",
    "        self.disc_loss = []\n",
    "        self.gen_loss =[]\n",
    "        \n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates note sequences\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        generated_seq = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(generated_seq)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
    "        model.add(Bidirectional(CuDNNLSTM(512)))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        seq = Input(shape=self.seq_shape)\n",
    "        validity = model(seq)\n",
    "\n",
    "        return Model(seq, validity)\n",
    "      \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.seq_shape))\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        seq = model(noise)\n",
    "\n",
    "        return Model(noise, seq)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        print('[train]: Starting...')\n",
    "        time_start = time.time()\n",
    "        # Load and convert the data\n",
    "        notes = get_notes()\n",
    "        n_vocab = len(set(notes))\n",
    "        X_train, y_train = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Insert saving; The generator image is saved every 500 steps\n",
    "        save_interval = 500\n",
    "\n",
    "        # Training the model\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Training the discriminator\n",
    "            # Select a random batch of note sequences\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_seqs = X_train[idx]\n",
    "\n",
    "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
    "            #noise = (noise-242)/242\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new note sequences\n",
    "            gen_seqs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            #  Training the Generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, real)\n",
    "\n",
    "\n",
    "\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "\n",
    "                # Save the generator model in a periodic basis\n",
    "                model_name = './pokemon_models/pokemon_' + str(epoch)\n",
    "                self.generator.save(model_name + \".h5\")\n",
    "\n",
    "                # Generate a midi file in a periodic basis\n",
    "                self.generate(notes, epoch)\n",
    "\n",
    "\n",
    "            # Print the progress and save into loss lists\n",
    "            if epoch % sample_interval == 0:\n",
    "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [time: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss, time.time() - time_start))\n",
    "              self.disc_loss.append(d_loss[0])\n",
    "              self.gen_loss.append(g_loss)\n",
    "        \n",
    "        self.generate(notes, epoch)\n",
    "        self.plot_loss()\n",
    "\n",
    "\n",
    "        time_end = time.time()\n",
    "        print('[train]: Ended. Time elapsed: {}'.format(time_end - time_start))\n",
    "        \n",
    "    def generate(self, input_notes, epoch):\n",
    "        # Get pitch names and store in a dictionary\n",
    "        notes = input_notes\n",
    "        pitchnames = sorted(set(item for item in notes))\n",
    "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "        \n",
    "        # Use random noise to generate sequences\n",
    "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
    "        predictions = self.generator.predict(noise)\n",
    "        \n",
    "        pred_notes = [x*242+242 for x in predictions[0]]\n",
    "        pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
    "        \n",
    "\n",
    "        file_name = './pokemon_output/pokemon_' + str(epoch)\n",
    "        create_midi(pred_notes, file_name)\n",
    "        time_end = time.time()\n",
    "        print('[generate midi]: Ended. Time elapsed: {}'.format(time_end - time_start))\n",
    "\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.disc_loss, c='red')\n",
    "        plt.plot(self.gen_loss, c='blue')\n",
    "        plt.title(\"GAN Loss per Epoch\")\n",
    "        plt.legend(['Discriminator', 'Generator'])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig('./pokemon_output/GAN_Loss_per_Epoch_final.png', transparent=True)\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  gan = GAN(rows=250)    \n",
    "  gan.train(epochs=500, batch_size=32, sample_interval=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coe197] *",
   "language": "python",
   "name": "conda-env-coe197-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
